# -*- coding: utf-8 -*-
"""
æ·±åº¦å­¦ä¹ é«˜çº§ä¼˜åŒ–ç®—æ³• - Adamã€RMSpropã€å­¦ä¹ ç‡è°ƒåº¦

åŒ…å«ï¼šAdamä¼˜åŒ–å™¨ã€RMSpropã€å­¦ä¹ ç‡è¡°å‡ã€æ‰¹å½’ä¸€åŒ–ã€Dropoutç­‰ä¼˜åŒ–æŠ€æœ¯ã€‚

æ³¨æ„: æ­¤æ–‡ä»¶å·²ä½œä¸ºå…¼å®¹å…¥å£ï¼Œæ¨èä½¿ç”¨ `deep_learning.optimizers` åŒ…ã€‚
"""

import warnings

# è½¬å‘åˆ°æ–°åŒ…å®ç°
from deep_learning.optimizers.advanced_optimization import *  # noqa: F401,F403

warnings.warn(
    "deep_learning_advanced_optimization.py å·²è¿ç§»åˆ° deep_learning/optimizers/ åŒ…ï¼Œ"
    "è¯·ä½¿ç”¨ deep_learning.optimizers ä¸‹çš„å¯¹åº”æ¨¡å—",
    DeprecationWarning,
    stacklevel=2,
)

def advanced_optimization_intro():
    """é«˜çº§ä¼˜åŒ–ç®—æ³•ä»‹ç»"""
    print("=== æ·±åº¦å­¦ä¹ é«˜çº§ä¼˜åŒ–ç®—æ³• ===")
    print("æ¢ç´¢æœ€æ–°çš„ä¼˜åŒ–æŠ€æœ¯å’Œç®—æ³•")
    print()
    print("æ¶µç›–å†…å®¹:")
    print("â€¢ äºŒé˜¶ä¼˜åŒ–æ–¹æ³• (Newton, Quasi-Newton)")
    print("â€¢ è‡ªé€‚åº”ä¼˜åŒ–ç®—æ³• (AdamW, Lookahead, RAdam)")
    print("â€¢ å­¦ä¹ ç‡è°ƒåº¦ç­–ç•¥ (Cosine Annealing, Warm Restart)")
    print("â€¢ æ¢¯åº¦ä¿®å‰ªä¸æ¢¯åº¦ç´¯ç§¯")
    print("â€¢ åˆ†å¸ƒå¼ä¼˜åŒ–ä¸å¹¶è¡Œè®­ç»ƒ")
    print("â€¢ å…ƒå­¦ä¹ ä¸ä¼˜åŒ–å™¨å­¦ä¹ ")
    print()

def second_order_optimization():
    """äºŒé˜¶ä¼˜åŒ–æ–¹æ³•"""
    print("\n" + "="*70)
    print("äºŒé˜¶ä¼˜åŒ–æ–¹æ³•")
    print("="*70)
    
    print("ç‰›é¡¿æ³•ä¸æ‹Ÿç‰›é¡¿æ³•:")
    print("â€¢ åˆ©ç”¨äºŒé˜¶ä¿¡æ¯åŠ é€Ÿæ”¶æ•›")
    print("â€¢ HessiançŸ©é˜µçš„è®¡ç®—ä¸è¿‘ä¼¼")
    print("â€¢ BFGSã€L-BFGSç®—æ³•")
    print("â€¢ åœ¨æ·±åº¦å­¦ä¹ ä¸­çš„åº”ç”¨æŒ‘æˆ˜")
    print()
    
    class SecondOrderOptimizer:
        """äºŒé˜¶ä¼˜åŒ–å™¨å®ç°"""
        
        def __init__(self, dim):
            self.dim = dim
            self.B = [[0.0 if i != j else 1.0 for j in range(dim)] for i in range(dim)]  # BFGSè¿‘ä¼¼Hessian
            self.history_s = []  # ä½ç½®å˜åŒ–å†å²
            self.history_y = []  # æ¢¯åº¦å˜åŒ–å†å²
            
        def newton_method(self, gradient, hessian):
            """ç‰›é¡¿æ³•æ›´æ–°æ­¥"""
            # Î”x = -H^(-1) * g
            try:
                # è®¡ç®—Hessiançš„é€†çŸ©é˜µ (ç®€åŒ–å®ç°)
                hessian_inv = self.matrix_inverse(hessian)
                update = self.matrix_vector_multiply(hessian_inv, gradient)
                return [-u for u in update]
            except:
                # å¦‚æœHessianä¸å¯é€†ï¼Œé€€åŒ–ä¸ºæ¢¯åº¦ä¸‹é™
                return [-g for g in gradient]
                
        def bfgs_update(self, s, y, rho_threshold=1e-6):
            """BFGSæ›´æ–°è¿‘ä¼¼Hessian"""
            # s = x_{k+1} - x_k, y = g_{k+1} - g_k
            
            # è®¡ç®— Ï = 1 / (y^T s)
            y_dot_s = sum(yi * si for yi, si in zip(y, s))
            
            if abs(y_dot_s) < rho_threshold:
                print(f"  è­¦å‘Š: Ï = {y_dot_s:.2e} è¿‡å°ï¼Œè·³è¿‡BFGSæ›´æ–°")
                return
                
            rho = 1.0 / y_dot_s
            
            # BFGSæ›´æ–°å…¬å¼
            # B_{k+1} = B_k - (B_k s s^T B_k)/(s^T B_k s) + (y y^T)/(y^T s)
            
            # è®¡ç®— B_k * s
            Bs = [sum(self.B[i][j] * s[j] for j in range(self.dim)) for i in range(self.dim)]
            
            # è®¡ç®— s^T * B_k * s  
            sBs = sum(s[i] * Bs[i] for i in range(self.dim))
            
            # æ›´æ–°BçŸ©é˜µ
            for i in range(self.dim):
                for j in range(self.dim):
                    # ç¬¬ä¸€é¡¹: B_k
                    term1 = self.B[i][j]
                    
                    # ç¬¬äºŒé¡¹: -(B_k s s^T B_k)/(s^T B_k s)
                    if sBs > 1e-12:
                        term2 = -(Bs[i] * Bs[j]) / sBs
                    else:
                        term2 = 0
                    
                    # ç¬¬ä¸‰é¡¹: (y y^T)/(y^T s)
                    term3 = (y[i] * y[j]) * rho
                    
                    self.B[i][j] = term1 + term2 + term3
        
        def lbfgs_direction(self, gradient, m=10):
            """L-BFGSæ–¹å‘è®¡ç®—"""
            # ä¿æŒæœ€è¿‘mæ¬¡çš„å†å²ä¿¡æ¯
            if len(self.history_s) > m:
                self.history_s = self.history_s[-m:]
                self.history_y = self.history_y[-m:]
            
            if not self.history_s:
                return [-g for g in gradient]
            
            # Two-loop recursion
            alphas = []
            q = gradient[:]
            
            # ç¬¬ä¸€ä¸ªå¾ªç¯ï¼šä»æ–°åˆ°æ—§
            for k in range(len(self.history_s)-1, -1, -1):
                s_k = self.history_s[k]
                y_k = self.history_y[k]
                
                rho_k = 1.0 / sum(y_k[i] * s_k[i] for i in range(self.dim))
                alpha_k = rho_k * sum(s_k[i] * q[i] for i in range(self.dim))
                
                for i in range(self.dim):
                    q[i] -= alpha_k * y_k[i]
                    
                alphas.append(alpha_k)
            
            alphas.reverse()
            
            # åˆå§‹Hessianè¿‘ä¼¼
            if self.history_y:
                s_newest = self.history_s[-1]
                y_newest = self.history_y[-1]
                gamma = (sum(s_newest[i] * y_newest[i] for i in range(self.dim)) / 
                        sum(y_newest[i] * y_newest[i] for i in range(self.dim)))
                r = [gamma * qi for qi in q]
            else:
                r = q[:]
            
            # ç¬¬äºŒä¸ªå¾ªç¯ï¼šä»æ—§åˆ°æ–°
            for k in range(len(self.history_s)):
                s_k = self.history_s[k]
                y_k = self.history_y[k]
                
                rho_k = 1.0 / sum(y_k[i] * s_k[i] for i in range(self.dim))
                beta_k = rho_k * sum(y_k[i] * r[i] for i in range(self.dim))
                
                for i in range(self.dim):
                    r[i] += s_k[i] * (alphas[k] - beta_k)
            
            return [-ri for ri in r]
        
        def matrix_inverse(self, matrix):
            """çŸ©é˜µæ±‚é€† (é«˜æ–¯-çº¦æ—¦æ¶ˆå…ƒæ³•)"""
            n = len(matrix)
            # åˆ›å»ºå¢å¹¿çŸ©é˜µ [A | I]
            augmented = []
            for i in range(n):
                row = matrix[i][:] + [0.0] * n
                row[n + i] = 1.0
                augmented.append(row)
            
            # é«˜æ–¯-çº¦æ—¦æ¶ˆå…ƒ
            for i in range(n):
                # æ‰¾ä¸»å…ƒ
                max_row = i
                for k in range(i + 1, n):
                    if abs(augmented[k][i]) > abs(augmented[max_row][i]):
                        max_row = k
                
                # äº¤æ¢è¡Œ
                if max_row != i:
                    augmented[i], augmented[max_row] = augmented[max_row], augmented[i]
                
                # ä¸»å…ƒå½’ä¸€åŒ–
                pivot = augmented[i][i]
                if abs(pivot) < 1e-10:
                    raise ValueError("çŸ©é˜µå¥‡å¼‚ï¼Œæ— æ³•æ±‚é€†")
                
                for j in range(2 * n):
                    augmented[i][j] /= pivot
                
                # æ¶ˆå…ƒ
                for k in range(n):
                    if k != i:
                        factor = augmented[k][i]
                        for j in range(2 * n):
                            augmented[k][j] -= factor * augmented[i][j]
            
            # æå–é€†çŸ©é˜µ
            inverse = []
            for i in range(n):
                inverse.append(augmented[i][n:])
            
            return inverse
        
        def matrix_vector_multiply(self, matrix, vector):
            """çŸ©é˜µå‘é‡ä¹˜æ³•"""
            result = []
            for row in matrix:
                dot_product = sum(a * b for a, b in zip(row, vector))
                result.append(dot_product)
            return result
    
    # äºŒé˜¶ä¼˜åŒ–æ¼”ç¤º
    print("äºŒé˜¶ä¼˜åŒ–ç®—æ³•æ¼”ç¤º:")
    
    def quadratic_function(x):
        """æµ‹è¯•å‡½æ•°: f(x) = x^T A x + b^T x + c"""
        A = [[2, 1], [1, 3]]
        b = [1, -1]
        c = 0
        
        result = c
        for i in range(len(b)):
            result += b[i] * x[i]
            
        for i in range(len(A)):
            for j in range(len(A[0])):
                result += 0.5 * A[i][j] * x[i] * x[j]
        
        return result
    
    def quadratic_gradient(x):
        """æ¢¯åº¦: g = Ax + b"""
        A = [[2, 1], [1, 3]]
        b = [1, -1]
        
        gradient = []
        for i in range(len(b)):
            gi = b[i]
            for j in range(len(A[0])):
                gi += A[i][j] * x[j]
            gradient.append(gi)
        
        return gradient
    
    def quadratic_hessian():
        """HessiançŸ©é˜µ (å¯¹äºäºŒæ¬¡å‡½æ•°æ˜¯å¸¸æ•°)"""
        return [[2, 1], [1, 3]]
    
    optimizer = SecondOrderOptimizer(2)
    x = [1.0, 1.0]  # åˆå§‹ç‚¹
    
    print(f"åˆå§‹ç‚¹: ({x[0]:.3f}, {x[1]:.3f})")
    print(f"åˆå§‹å‡½æ•°å€¼: {quadratic_function(x):.6f}")
    
    # ç‰›é¡¿æ³•ä¼˜åŒ–
    for iteration in range(3):
        grad = quadratic_gradient(x)
        hess = quadratic_hessian()
        
        update = optimizer.newton_method(grad, hess)
        
        print(f"\nè¿­ä»£ {iteration + 1}:")
        print(f"  æ¢¯åº¦: ({grad[0]:.6f}, {grad[1]:.6f})")
        print(f"  æ›´æ–°: ({update[0]:.6f}, {update[1]:.6f})")
        
        x = [x[i] + update[i] for i in range(len(x))]
        func_val = quadratic_function(x)
        
        print(f"  æ–°ä½ç½®: ({x[0]:.6f}, {x[1]:.6f})")
        print(f"  å‡½æ•°å€¼: {func_val:.10f}")
        
        # æ£€æŸ¥æ”¶æ•›
        grad_norm = math.sqrt(sum(g**2 for g in grad))
        if grad_norm < 1e-8:
            print(f"  å·²æ”¶æ•›ï¼æ¢¯åº¦èŒƒæ•°: {grad_norm:.2e}")
            break

def adaptive_learning_rate_methods():
    """è‡ªé€‚åº”å­¦ä¹ ç‡æ–¹æ³•"""
    print("\n" + "="*70)
    print("è‡ªé€‚åº”å­¦ä¹ ç‡ä¼˜åŒ–ç®—æ³•")
    print("="*70)
    
    print("ç°ä»£ä¼˜åŒ–ç®—æ³•:")
    print("â€¢ AdamW: Adam + Weight Decayè§£è€¦")
    print("â€¢ RAdam: Rectified Adam")
    print("â€¢ Lookahead: æ…¢æƒé‡æ›´æ–°æœºåˆ¶")
    print("â€¢ AdaBound: è‡ªé€‚åº”è¾¹ç•Œä¼˜åŒ–")
    print()
    
    class ModernOptimizers:
        """ç°ä»£ä¼˜åŒ–ç®—æ³•å®ç°"""
        
        def __init__(self, params_size):
            self.params_size = params_size
            self.reset()
            
        def reset(self):
            """é‡ç½®ä¼˜åŒ–å™¨çŠ¶æ€"""
            self.m = [0.0] * self.params_size     # ä¸€é˜¶åŠ¨é‡
            self.v = [0.0] * self.params_size     # äºŒé˜¶åŠ¨é‡
            self.t = 0                            # æ—¶é—´æ­¥
            self.slow_params = [0.0] * self.params_size  # Lookaheadæ…¢å‚æ•°
            
        def adamw_optimizer(self, params, gradients, lr=0.001, beta1=0.9, beta2=0.999, 
                           weight_decay=0.01, epsilon=1e-8):
            """AdamWä¼˜åŒ–å™¨ - Adam + Weight Decayè§£è€¦"""
            self.t += 1
            updated_params = []
            
            for i in range(len(params)):
                # æ›´æ–°åŠ¨é‡
                self.m[i] = beta1 * self.m[i] + (1 - beta1) * gradients[i]
                self.v[i] = beta2 * self.v[i] + (1 - beta2) * (gradients[i] ** 2)
                
                # åç½®ä¿®æ­£
                m_hat = self.m[i] / (1 - beta1 ** self.t)
                v_hat = self.v[i] / (1 - beta2 ** self.t)
                
                # AdamWæ›´æ–°ï¼šå…ˆåº”ç”¨æƒé‡è¡°å‡ï¼Œå†åº”ç”¨Adamæ›´æ–°
                param_decayed = params[i] * (1 - lr * weight_decay)
                adam_update = lr * m_hat / (math.sqrt(v_hat) + epsilon)
                
                new_param = param_decayed - adam_update
                updated_params.append(new_param)
                
            return updated_params
            
        def radam_optimizer(self, params, gradients, lr=0.001, beta1=0.9, beta2=0.999, 
                           epsilon=1e-8):
            """RAdam (Rectified Adam) ä¼˜åŒ–å™¨"""
            self.t += 1
            updated_params = []
            
            # è®¡ç®—Ï_âˆ (æ¸è¿‘å€¼)
            rho_inf = 2.0 / (1 - beta2) - 1
            
            for i in range(len(params)):
                # æ›´æ–°åŠ¨é‡
                self.m[i] = beta1 * self.m[i] + (1 - beta1) * gradients[i]
                self.v[i] = beta2 * self.v[i] + (1 - beta2) * (gradients[i] ** 2)
                
                # åç½®ä¿®æ­£
                m_hat = self.m[i] / (1 - beta1 ** self.t)
                
                # è®¡ç®—Ï_t
                rho_t = rho_inf - 2 * self.t * (beta2 ** self.t) / (1 - beta2 ** self.t)
                
                if rho_t > 4:  # ä½¿ç”¨ä¿®æ­£çš„è‡ªé€‚åº”å­¦ä¹ ç‡
                    v_hat = self.v[i] / (1 - beta2 ** self.t)
                    # è®¡ç®—ä¿®æ­£å› å­
                    l_t = math.sqrt((1 - beta2 ** self.t) / v_hat)
                    r_t = math.sqrt(((rho_t - 4) * (rho_t - 2) * rho_inf) / 
                                  ((rho_inf - 4) * (rho_inf - 2) * rho_t))
                    
                    update = lr * m_hat * r_t / (math.sqrt(v_hat) + epsilon)
                else:  # ä½¿ç”¨æ— ä¿®æ­£åŠ¨é‡
                    update = lr * m_hat
                
                updated_params.append(params[i] - update)
                
            return updated_params
            
        def lookahead_optimizer(self, params, fast_params, fast_gradients, alpha=0.5, k=5):
            """Lookaheadä¼˜åŒ–å™¨ - æ…¢æƒé‡æ›´æ–°"""
            # æ›´æ–°å¿«æƒé‡ (å¯ä»¥ä½¿ç”¨ä»»ä½•ä¼˜åŒ–å™¨)
            updated_fast_params = self.adamw_optimizer(fast_params, fast_gradients)
            
            # æ¯kæ­¥æ›´æ–°æ…¢æƒé‡
            if self.t % k == 0:
                print(f"    Lookaheadæ›´æ–° (æ­¥æ•°: {self.t})")
                for i in range(len(params)):
                    # Ï†_{t+1} = Ï†_t + Î±(Î¸_{t+1} - Ï†_t)
                    self.slow_params[i] = (self.slow_params[i] + 
                                         alpha * (updated_fast_params[i] - self.slow_params[i]))
                return self.slow_params[:], updated_fast_params
            else:
                return params[:], updated_fast_params
        
        def adabound_optimizer(self, params, gradients, lr=0.001, beta1=0.9, beta2=0.999,
                              final_lr=0.1, gamma=1e-3, epsilon=1e-8):
            """AdaBoundä¼˜åŒ–å™¨ - è‡ªé€‚åº”è¾¹ç•Œ"""
            self.t += 1
            updated_params = []
            
            for i in range(len(params)):
                # æ›´æ–°åŠ¨é‡
                self.m[i] = beta1 * self.m[i] + (1 - beta1) * gradients[i]
                self.v[i] = beta2 * self.v[i] + (1 - beta2) * (gradients[i] ** 2)
                
                # åç½®ä¿®æ­£
                m_hat = self.m[i] / (1 - beta1 ** self.t)
                v_hat = self.v[i] / (1 - beta2 ** self.t)
                
                # è®¡ç®—è‡ªé€‚åº”è¾¹ç•Œ
                lower_bound = final_lr * (1 - 1 / ((1 - beta2) * self.t + 1))
                upper_bound = final_lr * (1 + 1 / ((1 - beta2) * self.t))
                
                # è®¡ç®—æ­¥é•¿
                step_size = lr / math.sqrt(v_hat + epsilon)
                step_size = max(lower_bound, min(upper_bound, step_size))
                
                # å‚æ•°æ›´æ–°
                update = step_size * m_hat
                updated_params.append(params[i] - update)
                
            return updated_params
    
    # ç°ä»£ä¼˜åŒ–å™¨æ€§èƒ½æ¯”è¾ƒ
    print("ç°ä»£ä¼˜åŒ–å™¨æ€§èƒ½æ¯”è¾ƒ:")
    
    def himmelblau_function(x, y):
        """Himmelblauå‡½æ•° - å¤šå³°ä¼˜åŒ–æµ‹è¯•"""
        return (x**2 + y - 11)**2 + (x + y**2 - 7)**2
    
    def himmelblau_gradient(x, y):
        """Himmelblauå‡½æ•°æ¢¯åº¦"""
        dx = 4*x*(x**2 + y - 11) + 2*(x + y**2 - 7)
        dy = 2*(x**2 + y - 11) + 4*y*(x + y**2 - 7)
        return [dx, dy]
    
    # æµ‹è¯•ä¸åŒä¼˜åŒ–å™¨
    optimizers_config = [
        ("AdamW", "adamw_optimizer"),
        ("RAdam", "radam_optimizer"), 
        ("AdaBound", "adabound_optimizer")
    ]
    
    initial_params = [3.0, 3.0]
    print(f"æµ‹è¯•å‡½æ•°: Himmelblauå‡½æ•°")
    print(f"åˆå§‹ç‚¹: {initial_params}")
    print(f"è¿­ä»£æ¬¡æ•°: 100")
    print()
    
    for name, method_name in optimizers_config:
        optimizer = ModernOptimizers(2)
        params = initial_params[:]
        
        for iteration in range(100):
            grad = himmelblau_gradient(params[0], params[1])
            
            if method_name == "adamw_optimizer":
                params = optimizer.adamw_optimizer(params, grad, lr=0.01)
            elif method_name == "radam_optimizer":
                params = optimizer.radam_optimizer(params, grad, lr=0.01)
            elif method_name == "adabound_optimizer":
                params = optimizer.adabound_optimizer(params, grad, lr=0.01)
        
        final_value = himmelblau_function(params[0], params[1])
        print(f"{name:>10}: æœ€ç»ˆç‚¹({params[0]:7.4f}, {params[1]:7.4f}), å‡½æ•°å€¼={final_value:10.6f}")

def learning_rate_scheduling():
    """å­¦ä¹ ç‡è°ƒåº¦ç­–ç•¥"""
    print("\n" + "="*70)
    print("å­¦ä¹ ç‡è°ƒåº¦ç­–ç•¥")
    print("="*70)
    
    print("å­¦ä¹ ç‡è°ƒåº¦çš„é‡è¦æ€§:")
    print("â€¢ åˆæœŸï¼šå¤§å­¦ä¹ ç‡å¿«é€Ÿæ”¶æ•›")
    print("â€¢ åæœŸï¼šå°å­¦ä¹ ç‡ç²¾ç»†è°ƒæ•´")
    print("â€¢ é¿å…éœ‡è¡å’Œå‘æ•£")
    print("â€¢ æé«˜æœ€ç»ˆæ€§èƒ½")
    print()
    
    class LearningRateSchedulers:
        """å­¦ä¹ ç‡è°ƒåº¦å™¨é›†åˆ"""
        
        def __init__(self, initial_lr=0.1):
            self.initial_lr = initial_lr
            self.current_step = 0
            
        def step_decay(self, drop_rate=0.5, epochs_drop=10):
            """é˜¶æ®µè¡°å‡"""
            epoch = self.current_step // epochs_drop
            lr = self.initial_lr * (drop_rate ** epoch)
            return lr
            
        def exponential_decay(self, decay_rate=0.95):
            """æŒ‡æ•°è¡°å‡"""
            lr = self.initial_lr * (decay_rate ** self.current_step)
            return lr
            
        def cosine_annealing(self, T_max=100, eta_min=0.0001):
            """ä½™å¼¦é€€ç«"""
            lr = eta_min + (self.initial_lr - eta_min) * (
                1 + math.cos(math.pi * self.current_step / T_max)) / 2
            return lr
            
        def cosine_annealing_warm_restarts(self, T_0=10, T_mult=2, eta_min=0.0001):
            """å¸¦çƒ­é‡å¯çš„ä½™å¼¦é€€ç«"""
            if self.current_step == 0:
                return self.initial_lr
                
            # è®¡ç®—å½“å‰å‘¨æœŸ
            T_cur = T_0
            epoch_since_restart = self.current_step
            
            while epoch_since_restart >= T_cur:
                epoch_since_restart -= T_cur
                T_cur *= T_mult
            
            lr = eta_min + (self.initial_lr - eta_min) * (
                1 + math.cos(math.pi * epoch_since_restart / T_cur)) / 2
            
            return lr
            
        def polynomial_decay(self, max_steps=1000, power=1.0, end_lr=0.0001):
            """å¤šé¡¹å¼è¡°å‡"""
            if self.current_step >= max_steps:
                return end_lr
            
            decay_factor = (1 - self.current_step / max_steps) ** power
            lr = (self.initial_lr - end_lr) * decay_factor + end_lr
            return lr
            
        def warmup_cosine(self, warmup_steps=100, total_steps=1000):
            """é¢„çƒ­ + ä½™å¼¦è¡°å‡"""
            if self.current_step < warmup_steps:
                # çº¿æ€§é¢„çƒ­
                lr = self.initial_lr * self.current_step / warmup_steps
            else:
                # ä½™å¼¦è¡°å‡
                progress = (self.current_step - warmup_steps) / (total_steps - warmup_steps)
                progress = min(progress, 1.0)
                lr = 0.5 * self.initial_lr * (1 + math.cos(math.pi * progress))
            
            return lr
        
        def step(self):
            """æ›´æ–°æ­¥æ•°"""
            self.current_step += 1
    
    # å­¦ä¹ ç‡è°ƒåº¦æ¼”ç¤º
    print("å­¦ä¹ ç‡è°ƒåº¦ç­–ç•¥æ¼”ç¤º:")
    
    scheduler = LearningRateSchedulers(initial_lr=0.1)
    total_steps = 200
    
    # æ”¶é›†ä¸åŒè°ƒåº¦ç­–ç•¥çš„å­¦ä¹ ç‡
    schedules = {
        "é˜¶æ®µè¡°å‡": [],
        "æŒ‡æ•°è¡°å‡": [],
        "ä½™å¼¦é€€ç«": [],
        "ä½™å¼¦çƒ­é‡å¯": [],
        "é¢„çƒ­+ä½™å¼¦": []
    }
    
    for step in range(total_steps):
        scheduler.current_step = step
        
        schedules["é˜¶æ®µè¡°å‡"].append(scheduler.step_decay())
        schedules["æŒ‡æ•°è¡°å‡"].append(scheduler.exponential_decay())
        schedules["ä½™å¼¦é€€ç«"].append(scheduler.cosine_annealing(T_max=total_steps))
        schedules["ä½™å¼¦çƒ­é‡å¯"].append(scheduler.cosine_annealing_warm_restarts())
        schedules["é¢„çƒ­+ä½™å¼¦"].append(scheduler.warmup_cosine(total_steps=total_steps))
    
    # æ˜¾ç¤ºå…³é”®æ­¥æ•°çš„å­¦ä¹ ç‡
    key_steps = [0, 20, 50, 100, 150, 199]
    print(f"{'ç­–ç•¥':>12} | " + " | ".join(f"æ­¥æ•°{s:>3}" for s in key_steps))
    print("-" * (12 + len(key_steps) * 9))
    
    for name, lr_values in schedules.items():
        lr_at_steps = [lr_values[s] for s in key_steps]
        print(f"{name:>12} | " + " | ".join(f"{lr:>6.4f}" for lr in lr_at_steps))
    
    print(f"\nå­¦ä¹ ç‡è°ƒåº¦ç­–ç•¥é€‰æ‹©æŒ‡å—:")
    print(f"â€¢ é˜¶æ®µè¡°å‡: ç®€å•æœ‰æ•ˆï¼Œéœ€è¦æ‰‹åŠ¨è®¾ç½®è¡°å‡ç‚¹")
    print(f"â€¢ æŒ‡æ•°è¡°å‡: å¹³æ»‘è¡°å‡ï¼Œä½†å¯èƒ½è¡°å‡è¿‡å¿«")
    print(f"â€¢ ä½™å¼¦é€€ç«: è‡ªç„¶çš„è¡°å‡æ›²çº¿ï¼Œå¹¿æ³›ä½¿ç”¨")
    print(f"â€¢ ä½™å¼¦çƒ­é‡å¯: é¿å…å±€éƒ¨æœ€ä¼˜ï¼Œé€‚åˆé•¿è®­ç»ƒ")
    print(f"â€¢ é¢„çƒ­+ä½™å¼¦: ç°ä»£è®­ç»ƒçš„æ ‡å‡†é…ç½®")

def gradient_clipping_and_accumulation():
    """æ¢¯åº¦è£å‰ªä¸æ¢¯åº¦ç´¯ç§¯"""
    print("\n" + "="*70)
    print("æ¢¯åº¦è£å‰ªä¸æ¢¯åº¦ç´¯ç§¯")
    print("="*70)
    
    print("æ¢¯åº¦è£å‰ªçš„å¿…è¦æ€§:")
    print("â€¢ é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸")
    print("â€¢ ç¨³å®šè®­ç»ƒè¿‡ç¨‹")
    print("â€¢ ç‰¹åˆ«é‡è¦äºRNNè®­ç»ƒ")
    print()
    
    print("æ¢¯åº¦ç´¯ç§¯çš„åº”ç”¨:")
    print("â€¢ æ¨¡æ‹Ÿå¤§æ‰¹é‡è®­ç»ƒ")
    print("â€¢ èŠ‚çœæ˜¾å­˜èµ„æº")
    print("â€¢ æé«˜è®­ç»ƒç¨³å®šæ€§")
    print()
    
    class GradientProcessing:
        """æ¢¯åº¦å¤„ç†å·¥å…·"""
        
        def __init__(self):
            self.accumulated_gradients = []
            self.accumulation_steps = 0
            
        def gradient_clipping_norm(self, gradients, max_norm=1.0):
            """æŒ‰èŒƒæ•°è£å‰ªæ¢¯åº¦"""
            # è®¡ç®—æ¢¯åº¦çš„L2èŒƒæ•°
            grad_norm = math.sqrt(sum(g**2 for g in gradients))
            
            if grad_norm > max_norm:
                # ç¼©æ”¾æ¢¯åº¦
                scale_factor = max_norm / grad_norm
                clipped_gradients = [g * scale_factor for g in gradients]
                
                print(f"  æ¢¯åº¦è£å‰ª: åŸèŒƒæ•°={grad_norm:.4f}, è£å‰ªå={max_norm:.4f}")
                return clipped_gradients, True
            else:
                return gradients[:], False
                
        def gradient_clipping_value(self, gradients, max_value=0.5):
            """æŒ‰å€¼è£å‰ªæ¢¯åº¦"""
            clipped_gradients = []
            clipped_count = 0
            
            for g in gradients:
                if g > max_value:
                    clipped_gradients.append(max_value)
                    clipped_count += 1
                elif g < -max_value:
                    clipped_gradients.append(-max_value)
                    clipped_count += 1
                else:
                    clipped_gradients.append(g)
            
            if clipped_count > 0:
                print(f"  å€¼è£å‰ª: {clipped_count} ä¸ªæ¢¯åº¦è¢«è£å‰ªåˆ° Â±{max_value}")
                
            return clipped_gradients, clipped_count > 0
            
        def gradient_accumulation(self, gradients, accumulation_steps=4):
            """æ¢¯åº¦ç´¯ç§¯"""
            if len(self.accumulated_gradients) == 0:
                self.accumulated_gradients = [0.0] * len(gradients)
            
            # ç´¯ç§¯å½“å‰æ¢¯åº¦
            for i in range(len(gradients)):
                self.accumulated_gradients[i] += gradients[i]
            
            self.accumulation_steps += 1
            
            # æ£€æŸ¥æ˜¯å¦åˆ°è¾¾ç´¯ç§¯æ­¥æ•°
            if self.accumulation_steps >= accumulation_steps:
                # è®¡ç®—å¹³å‡æ¢¯åº¦
                averaged_gradients = [g / accumulation_steps for g in self.accumulated_gradients]
                
                # é‡ç½®ç´¯ç§¯çŠ¶æ€
                self.accumulated_gradients = [0.0] * len(gradients)
                self.accumulation_steps = 0
                
                return averaged_gradients, True  # è¿”å›å¹³å‡æ¢¯åº¦å’Œæ›´æ–°æ ‡å¿—
            else:
                return None, False  # è¿˜æœªè¾¾åˆ°æ›´æ–°æ¡ä»¶
        
        def adaptive_gradient_clipping(self, gradients, parameters, percentile=10):
            """è‡ªé€‚åº”æ¢¯åº¦è£å‰ª"""
            # è®¡ç®—å‚æ•°çš„èŒƒæ•°
            param_norm = math.sqrt(sum(p**2 for p in parameters))
            grad_norm = math.sqrt(sum(g**2 for g in gradients))
            
            if param_norm == 0 or grad_norm == 0:
                return gradients[:]
            
            # æ ¹æ®å‚æ•°èŒƒæ•°è‡ªé€‚åº”è°ƒæ•´è£å‰ªé˜ˆå€¼
            max_norm = param_norm * percentile / 100.0
            
            if grad_norm > max_norm:
                scale_factor = max_norm / grad_norm
                clipped_gradients = [g * scale_factor for g in gradients]
                
                print(f"  è‡ªé€‚åº”è£å‰ª: å‚æ•°èŒƒæ•°={param_norm:.4f}, æ¢¯åº¦èŒƒæ•°={grad_norm:.4f} -> {max_norm:.4f}")
                return clipped_gradients
            else:
                return gradients[:]
    
    # æ¢¯åº¦å¤„ç†æ¼”ç¤º
    print("æ¢¯åº¦å¤„ç†æŠ€æœ¯æ¼”ç¤º:")
    
    grad_processor = GradientProcessing()
    
    # æ¨¡æ‹Ÿè®­ç»ƒè¿‡ç¨‹ä¸­çš„æ¢¯åº¦
    training_gradients = [
        [0.1, 0.2, -0.15],      # æ­£å¸¸æ¢¯åº¦
        [2.5, -1.8, 3.2],      # çˆ†ç‚¸æ¢¯åº¦
        [0.05, 0.08, -0.03],   # å°æ¢¯åº¦
        [1.2, -0.9, 1.5],      # ä¸­ç­‰æ¢¯åº¦
        [0.3, 0.1, -0.2],      # æ­£å¸¸æ¢¯åº¦
    ]
    
    parameters = [1.0, 0.5, -0.8]  # æ¨¡æ‹Ÿå‚æ•°
    
    print(f"åŸå§‹æ¢¯åº¦åºåˆ—:")
    for i, grad in enumerate(training_gradients):
        grad_norm = math.sqrt(sum(g**2 for g in grad))
        print(f"  æ­¥éª¤{i+1}: {grad} (èŒƒæ•°: {grad_norm:.4f})")
    
    print(f"\n1. èŒƒæ•°æ¢¯åº¦è£å‰ª (max_norm=1.0):")
    for i, grad in enumerate(training_gradients):
        clipped_grad, was_clipped = grad_processor.gradient_clipping_norm(grad, max_norm=1.0)
        if was_clipped:
            print(f"  æ­¥éª¤{i+1}: {[f'{g:.4f}' for g in clipped_grad]}")
        else:
            print(f"  æ­¥éª¤{i+1}: æœªè£å‰ª")
    
    print(f"\n2. è‡ªé€‚åº”æ¢¯åº¦è£å‰ª:")
    for i, grad in enumerate(training_gradients):
        clipped_grad = grad_processor.adaptive_gradient_clipping(grad, parameters)
        original_norm = math.sqrt(sum(g**2 for g in grad))
        clipped_norm = math.sqrt(sum(g**2 for g in clipped_grad))
        if abs(original_norm - clipped_norm) > 1e-6:
            print(f"  æ­¥éª¤{i+1}: èŒƒæ•° {original_norm:.4f} -> {clipped_norm:.4f}")
    
    print(f"\n3. æ¢¯åº¦ç´¯ç§¯æ¼”ç¤º (ç´¯ç§¯4æ­¥):")
    grad_processor = GradientProcessing()  # é‡ç½®
    for i, grad in enumerate(training_gradients):
        avg_grad, should_update = grad_processor.gradient_accumulation(grad, accumulation_steps=2)
        
        if should_update:
            print(f"  ç´¯ç§¯å®Œæˆï¼Œå¹³å‡æ¢¯åº¦: {[f'{g:.4f}' for g in avg_grad]}")
        else:
            print(f"  æ­¥éª¤{i+1}: ç´¯ç§¯ä¸­... (å·²ç´¯ç§¯{grad_processor.accumulation_steps}æ­¥)")

def main():
    """ä¸»å‡½æ•°"""
    print("âš¡ æ·±åº¦å­¦ä¹ é«˜çº§ä¼˜åŒ–ç®—æ³•")
    print("=" * 70)
    
    advanced_optimization_intro()
    second_order_optimization()
    adaptive_learning_rate_methods()
    learning_rate_scheduling()
    gradient_clipping_and_accumulation()
    
    print("\n" + "=" * 70)
    print("ğŸ¯ é«˜çº§ä¼˜åŒ–æŠ€æœ¯æ€»ç»“")
    print()
    print("æŒæ¡çš„ä¼˜åŒ–æŠ€æœ¯:")
    print("â€¢ äºŒé˜¶ä¼˜åŒ–æ–¹æ³•ï¼šç‰›é¡¿æ³•ã€BFGSã€L-BFGS")
    print("â€¢ ç°ä»£è‡ªé€‚åº”ç®—æ³•ï¼šAdamWã€RAdamã€Lookahead")
    print("â€¢ å­¦ä¹ ç‡è°ƒåº¦ï¼šä½™å¼¦é€€ç«ã€é¢„çƒ­ç­–ç•¥")
    print("â€¢ æ¢¯åº¦å¤„ç†ï¼šè£å‰ªã€ç´¯ç§¯ã€è‡ªé€‚åº”æŠ€æœ¯")
    print()
    print("è¿™äº›é«˜çº§ä¼˜åŒ–æŠ€æœ¯æ˜¯è®­ç»ƒå¤§å‹æ·±åº¦å­¦ä¹ ")
    print("æ¨¡å‹çš„å…³é”®å·¥å…·ï¼ŒæŒæ¡å®ƒä»¬å°†æ˜¾è‘—æå‡")
    print("ä½ çš„æ¨¡å‹è®­ç»ƒæ•ˆæœå’Œæ•ˆç‡ï¼")

if __name__ == "__main__":
    main()
