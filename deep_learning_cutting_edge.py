# -*- coding: utf-8 -*-
"""
å‰æ²¿æ·±åº¦å­¦ä¹ æ¶æ„ - Transformerã€ViTã€EfficientNetã€MoE

åŒ…å«ï¼šTransformerè¯¦ç»†å®ç°ã€Vision Transformerã€EfficientNetã€æ··åˆä¸“å®¶æ¨¡å‹ç­‰å‰æ²¿æ¶æ„ã€‚
"""
# Cutting-Edge Deep Learning Architectures: æœ€æ–°æ¶æ„å®ç°ä¸åŸç†

import random
import math
import json

def cutting_edge_intro():
    """å‰æ²¿æ¶æ„ä»‹ç»"""
    print("=== å‰æ²¿æ·±åº¦å­¦ä¹ æ¶æ„ ===")
    print("æ¢ç´¢æœ€æ–°çš„ç¥ç»ç½‘ç»œæ¶æ„å’ŒæŠ€æœ¯")
    print()
    print("å‰æ²¿æ¶æ„:")
    print("â€¢ Vision Transformer (ViT)")
    print("â€¢ BERTä¸GPTç³»åˆ—æ¨¡å‹")
    print("â€¢ EfficientNetä¸ç¥ç»æ¶æ„æœç´¢")
    print("â€¢ æ®‹å·®ç½‘ç»œçš„è¿›åŒ– (ResNeXt, DenseNet)")
    print("â€¢ æ³¨æ„åŠ›æœºåˆ¶çš„å˜ç§ (Self-Attention, Cross-Attention)")
    print("â€¢ æ··åˆä¸“å®¶æ¨¡å‹ (Mixture of Experts)")
    print("â€¢ ç¥ç»ODEä¸è¿ç»­æ·±åº¦æ¨¡å‹")
    print()

def transformer_detailed_implementation():
    """Transformerè¯¦ç»†å®ç°"""
    print("\n" + "="*70)
    print("Transformeræ¶æ„æ·±åº¦å®ç°")
    print("="*70)
    
    print("Transformeræ ¸å¿ƒç»„ä»¶:")
    print("â€¢ Multi-Head Self-Attention")
    print("â€¢ Position Encoding")
    print("â€¢ Feed-Forward Networks")
    print("â€¢ Layer Normalization")
    print("â€¢ Residual Connections")
    print()
    
    class MultiHeadAttention:
        """å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶å®ç°"""
        
        def __init__(self, d_model=512, num_heads=8):
            self.d_model = d_model
            self.num_heads = num_heads
            self.d_k = d_model // num_heads
            
            # åˆå§‹åŒ–æƒé‡çŸ©é˜µ
            self.W_q = self._init_weights(d_model, d_model)
            self.W_k = self._init_weights(d_model, d_model)
            self.W_v = self._init_weights(d_model, d_model)
            self.W_o = self._init_weights(d_model, d_model)
            
        def _init_weights(self, in_dim, out_dim):
            """Xavieråˆå§‹åŒ–"""
            limit = math.sqrt(6.0 / (in_dim + out_dim))
            return [[random.uniform(-limit, limit) for _ in range(out_dim)] 
                   for _ in range(in_dim)]
        
        def scaled_dot_product_attention(self, Q, K, V, mask=None):
            """ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›"""
            # è®¡ç®—æ³¨æ„åŠ›åˆ†æ•° scores = Q @ K.T / sqrt(d_k)
            seq_len = len(Q)
            scores = []
            
            for i in range(seq_len):
                score_row = []
                for j in range(seq_len):
                    score = sum(Q[i][k] * K[j][k] for k in range(self.d_k))
                    score /= math.sqrt(self.d_k)
                    score_row.append(score)
                scores.append(score_row)
            
            # åº”ç”¨mask (å¦‚æœæœ‰)
            if mask is not None:
                for i in range(seq_len):
                    for j in range(seq_len):
                        if mask[i][j] == 0:
                            scores[i][j] = float('-inf')
            
            # Softmax
            attention_weights = []
            for i in range(seq_len):
                # æ•°å€¼ç¨³å®šçš„softmax
                max_score = max(scores[i])
                exp_scores = [math.exp(s - max_score) for s in scores[i]]
                sum_exp = sum(exp_scores)
                weights = [exp_s / sum_exp for exp_s in exp_scores]
                attention_weights.append(weights)
            
            # è®¡ç®—è¾“å‡º output = attention_weights @ V
            output = []
            for i in range(seq_len):
                output_vector = [0.0] * self.d_k
                for j in range(seq_len):
                    for k in range(self.d_k):
                        output_vector[k] += attention_weights[i][j] * V[j][k]
                output.append(output_vector)
            
            return output, attention_weights
        
        def forward(self, x, mask=None):
            """å¤šå¤´æ³¨æ„åŠ›å‰å‘ä¼ æ’­"""
            seq_len = len(x)
            
            # çº¿æ€§å˜æ¢å¾—åˆ°Q, K, V
            Q = self._linear_transform(x, self.W_q)
            K = self._linear_transform(x, self.W_k)
            V = self._linear_transform(x, self.W_v)
            
            # åˆ†å‰²ä¸ºå¤šä¸ªå¤´
            Q_heads = self._split_heads(Q)
            K_heads = self._split_heads(K)
            V_heads = self._split_heads(V)
            
            # å¯¹æ¯ä¸ªå¤´è®¡ç®—æ³¨æ„åŠ›
            attention_outputs = []
            all_attention_weights = []
            
            for h in range(self.num_heads):
                output, weights = self.scaled_dot_product_attention(
                    Q_heads[h], K_heads[h], V_heads[h], mask)
                attention_outputs.append(output)
                all_attention_weights.append(weights)
            
            # è¿æ¥æ‰€æœ‰å¤´çš„è¾“å‡º
            concatenated = self._concat_heads(attention_outputs)
            
            # æœ€ç»ˆçº¿æ€§å˜æ¢
            final_output = self._linear_transform(concatenated, self.W_o)
            
            return final_output, all_attention_weights
        
        def _linear_transform(self, x, W):
            """çº¿æ€§å˜æ¢ x @ W"""
            seq_len = len(x)
            d_in = len(x[0])
            d_out = len(W[0])
            
            output = []
            for i in range(seq_len):
                output_vector = []
                for j in range(d_out):
                    value = sum(x[i][k] * W[k][j] for k in range(d_in))
                    output_vector.append(value)
                output.append(output_vector)
            
            return output
        
        def _split_heads(self, x):
            """å°†è¾“å…¥åˆ†å‰²ä¸ºå¤šä¸ªå¤´"""
            seq_len = len(x)
            heads = []
            
            for h in range(self.num_heads):
                head_data = []
                for i in range(seq_len):
                    head_vector = x[i][h * self.d_k:(h + 1) * self.d_k]
                    head_data.append(head_vector)
                heads.append(head_data)
            
            return heads
        
        def _concat_heads(self, heads):
            """è¿æ¥å¤šä¸ªå¤´çš„è¾“å‡º"""
            seq_len = len(heads[0])
            concatenated = []
            
            for i in range(seq_len):
                concat_vector = []
                for h in range(self.num_heads):
                    concat_vector.extend(heads[h][i])
                concatenated.append(concat_vector)
            
            return concatenated
    
    class PositionalEncoding:
        """ä½ç½®ç¼–ç """
        
        def __init__(self, d_model=512, max_len=5000):
            self.d_model = d_model
            self.max_len = max_len
            self.encoding = self._generate_encoding()
        
        def _generate_encoding(self):
            """ç”Ÿæˆæ­£å¼¦ä½ç½®ç¼–ç """
            encoding = []
            
            for pos in range(self.max_len):
                pos_encoding = []
                for i in range(self.d_model):
                    if i % 2 == 0:
                        # å¶æ•°ä½ç½®ä½¿ç”¨sin
                        angle = pos / (10000 ** (i / self.d_model))
                        pos_encoding.append(math.sin(angle))
                    else:
                        # å¥‡æ•°ä½ç½®ä½¿ç”¨cos
                        angle = pos / (10000 ** ((i-1) / self.d_model))
                        pos_encoding.append(math.cos(angle))
                encoding.append(pos_encoding)
            
            return encoding
        
        def add_positional_encoding(self, x):
            """æ·»åŠ ä½ç½®ç¼–ç åˆ°è¾“å…¥"""
            seq_len = len(x)
            encoded = []
            
            for i in range(seq_len):
                encoded_vector = []
                for j in range(len(x[i])):
                    encoded_value = x[i][j] + self.encoding[i][j]
                    encoded_vector.append(encoded_value)
                encoded.append(encoded_vector)
            
            return encoded
    
    class TransformerBlock:
        """Transformerå—"""
        
        def __init__(self, d_model=512, num_heads=8, d_ff=2048, dropout=0.1):
            self.d_model = d_model
            self.d_ff = d_ff
            self.dropout = dropout
            
            self.attention = MultiHeadAttention(d_model, num_heads)
            self.pos_encoding = PositionalEncoding(d_model)
            
            # å‰é¦ˆç½‘ç»œæƒé‡
            self.W1 = self._init_weights(d_model, d_ff)
            self.b1 = [0.0] * d_ff
            self.W2 = self._init_weights(d_ff, d_model)
            self.b2 = [0.0] * d_model
            
        def _init_weights(self, in_dim, out_dim):
            """Xavieråˆå§‹åŒ–"""
            limit = math.sqrt(6.0 / (in_dim + out_dim))
            return [[random.uniform(-limit, limit) for _ in range(out_dim)] 
                   for _ in range(in_dim)]
        
        def layer_norm(self, x, eps=1e-6):
            """å±‚å½’ä¸€åŒ–"""
            normalized = []
            
            for vector in x:
                # è®¡ç®—å‡å€¼å’Œæ–¹å·®
                mean = sum(vector) / len(vector)
                variance = sum((v - mean) ** 2 for v in vector) / len(vector)
                
                # å½’ä¸€åŒ–
                norm_vector = [(v - mean) / math.sqrt(variance + eps) for v in vector]
                normalized.append(norm_vector)
            
            return normalized
        
        def feed_forward(self, x):
            """å‰é¦ˆç½‘ç»œ"""
            # ç¬¬ä¸€å±‚: ReLU(x @ W1 + b1)
            hidden = []
            for vector in x:
                hidden_vector = []
                for i in range(self.d_ff):
                    value = sum(vector[j] * self.W1[j][i] for j in range(len(vector))) + self.b1[i]
                    hidden_vector.append(max(0, value))  # ReLU
                hidden.append(hidden_vector)
            
            # ç¬¬äºŒå±‚: hidden @ W2 + b2
            output = []
            for vector in hidden:
                output_vector = []
                for i in range(self.d_model):
                    value = sum(vector[j] * self.W2[j][i] for j in range(len(vector))) + self.b2[i]
                    output_vector.append(value)
                output.append(output_vector)
            
            return output
        
        def forward(self, x, mask=None):
            """Transformerå—å‰å‘ä¼ æ’­"""
            # æ·»åŠ ä½ç½®ç¼–ç 
            x_pos = self.pos_encoding.add_positional_encoding(x)
            
            # Multi-Head Self-Attention + æ®‹å·®è¿æ¥ + å±‚å½’ä¸€åŒ–
            attn_output, attn_weights = self.attention.forward(x_pos, mask)
            
            # æ®‹å·®è¿æ¥
            x1 = []
            for i in range(len(x_pos)):
                residual_vector = []
                for j in range(len(x_pos[i])):
                    residual_vector.append(x_pos[i][j] + attn_output[i][j])
                x1.append(residual_vector)
            
            # å±‚å½’ä¸€åŒ–
            x1_norm = self.layer_norm(x1)
            
            # Feed-Forward + æ®‹å·®è¿æ¥ + å±‚å½’ä¸€åŒ–
            ff_output = self.feed_forward(x1_norm)
            
            # æ®‹å·®è¿æ¥
            x2 = []
            for i in range(len(x1_norm)):
                residual_vector = []
                for j in range(len(x1_norm[i])):
                    residual_vector.append(x1_norm[i][j] + ff_output[i][j])
                x2.append(residual_vector)
            
            # å±‚å½’ä¸€åŒ–
            output = self.layer_norm(x2)
            
            return output, attn_weights
    
    # Transformeræ¼”ç¤º
    print("Transformeræ¶æ„æ¼”ç¤º:")
    
    # åˆ›å»ºæ¨¡æ‹Ÿè¾“å…¥ (seq_len=4, d_model=8 ä¸ºäº†æ¼”ç¤º)
    seq_len = 4
    d_model = 8
    
    # éšæœºè¾“å…¥åºåˆ—
    input_sequence = []
    for i in range(seq_len):
        vector = [random.uniform(-1, 1) for _ in range(d_model)]
        input_sequence.append(vector)
    
    print(f"è¾“å…¥åºåˆ—å½¢çŠ¶: ({seq_len}, {d_model})")
    print(f"å‰3ä¸ªä½ç½®çš„è¾“å…¥:")
    for i in range(min(3, seq_len)):
        formatted_vector = [f"{v:.3f}" for v in input_sequence[i]]
        print(f"  ä½ç½®{i}: [{', '.join(formatted_vector)}]")
    
    # åˆ›å»ºTransformerå—
    transformer = TransformerBlock(d_model=d_model, num_heads=2, d_ff=16)
    
    # å‰å‘ä¼ æ’­
    output, attention_weights = transformer.forward(input_sequence)
    
    print(f"\nTransformerè¾“å‡º:")
    for i in range(min(3, seq_len)):
        formatted_output = [f"{v:.3f}" for v in output[i]]
        print(f"  ä½ç½®{i}: [{', '.join(formatted_output)}]")
    
    print(f"\næ³¨æ„åŠ›æƒé‡çŸ©é˜µ (å¤´1):")
    for i in range(seq_len):
        weights_str = [f"{w:.3f}" for w in attention_weights[0][i]]
        print(f"  ä½ç½®{i}: [{', '.join(weights_str)}]")
    
    print(f"\nTransformerå…³é”®ç‰¹æ€§:")
    print(f"â€¢ è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼šæ¯ä¸ªä½ç½®å…³æ³¨æ‰€æœ‰ä½ç½®")
    print(f"â€¢ ä½ç½®ç¼–ç ï¼šä¸ºåºåˆ—æ·»åŠ ä½ç½®ä¿¡æ¯")
    print(f"â€¢ æ®‹å·®è¿æ¥ï¼šç¼“è§£æ¢¯åº¦æ¶ˆå¤±é—®é¢˜")
    print(f"â€¢ å±‚å½’ä¸€åŒ–ï¼šç¨³å®šè®­ç»ƒè¿‡ç¨‹")

def vision_transformer_implementation():
    """Vision Transformerå®ç°"""
    print("\n" + "="*70)
    print("Vision Transformer (ViT)")
    print("="*70)
    
    print("ViTæ ¸å¿ƒæ€æƒ³:")
    print("â€¢ å°†å›¾åƒåˆ†å‰²ä¸ºpatches")
    print("â€¢ æ¯ä¸ªpatchè§†ä¸ºåºåˆ—ä¸­çš„token")
    print("â€¢ åº”ç”¨æ ‡å‡†Transformeræ¶æ„")
    print("â€¢ æ·»åŠ å¯å­¦ä¹ çš„åˆ†ç±»token")
    print()
    
    class VisionTransformer:
        """Vision Transformerå®ç°"""
        
        def __init__(self, image_size=224, patch_size=16, num_classes=1000, 
                     d_model=768, num_heads=12, num_layers=12):
            self.image_size = image_size
            self.patch_size = patch_size
            self.num_classes = num_classes
            self.d_model = d_model
            self.num_heads = num_heads
            self.num_layers = num_layers
            
            # è®¡ç®—patchæ•°é‡
            self.num_patches = (image_size // patch_size) ** 2
            self.seq_len = self.num_patches + 1  # +1 for class token
            
            # åˆå§‹åŒ–å‚æ•°
            self.patch_embedding = self._init_patch_embedding()
            self.class_token = [random.gauss(0, 0.02) for _ in range(d_model)]
            self.position_embeddings = self._init_position_embeddings()
            
            print(f"ViTé…ç½®:")
            print(f"  å›¾åƒå¤§å°: {image_size}x{image_size}")
            print(f"  Patchå¤§å°: {patch_size}x{patch_size}")
            print(f"  Patchæ•°é‡: {self.num_patches}")
            print(f"  åºåˆ—é•¿åº¦: {self.seq_len} (åŒ…å«class token)")
            print(f"  æ¨¡å‹ç»´åº¦: {d_model}")
        
        def _init_patch_embedding(self):
            """åˆå§‹åŒ–patchåµŒå…¥æƒé‡"""
            input_dim = self.patch_size * self.patch_size * 3  # RGB channels
            limit = math.sqrt(6.0 / (input_dim + self.d_model))
            return [[random.uniform(-limit, limit) for _ in range(self.d_model)] 
                   for _ in range(input_dim)]
        
        def _init_position_embeddings(self):
            """åˆå§‹åŒ–ä½ç½®åµŒå…¥"""
            embeddings = []
            for i in range(self.seq_len):
                embedding = [random.gauss(0, 0.02) for _ in range(self.d_model)]
                embeddings.append(embedding)
            return embeddings
        
        def image_to_patches(self, image):
            """å°†å›¾åƒè½¬æ¢ä¸ºpatches"""
            # æ¨¡æ‹Ÿå›¾åƒåˆ‡åˆ†è¿‡ç¨‹
            patches = []
            
            for i in range(0, self.image_size, self.patch_size):
                for j in range(0, self.image_size, self.patch_size):
                    # æå–patch (ç®€åŒ–ä¸ºéšæœºå€¼)
                    patch = []
                    for c in range(3):  # RGB
                        for pi in range(self.patch_size):
                            for pj in range(self.patch_size):
                                # åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œåº”è¯¥æ˜¯ image[i+pi][j+pj][c]
                                patch.append(random.uniform(0, 1))
                    patches.append(patch)
            
            return patches
        
        def patch_embedding_forward(self, patches):
            """PatchåµŒå…¥"""
            embedded_patches = []
            
            for patch in patches:
                embedded = []
                for i in range(self.d_model):
                    value = sum(patch[j] * self.patch_embedding[j][i] 
                              for j in range(len(patch)))
                    embedded.append(value)
                embedded_patches.append(embedded)
            
            return embedded_patches
        
        def add_class_token_and_position(self, embedded_patches):
            """æ·»åŠ class tokenå’Œä½ç½®åµŒå…¥"""
            # æ·»åŠ class tokenåˆ°åºåˆ—å¼€å¤´
            sequence = [self.class_token[:]] + embedded_patches
            
            # æ·»åŠ ä½ç½®åµŒå…¥
            for i in range(len(sequence)):
                for j in range(self.d_model):
                    sequence[i][j] += self.position_embeddings[i][j]
            
            return sequence
        
        def forward(self, image):
            """ViTå‰å‘ä¼ æ’­"""
            # 1. å›¾åƒåˆ°patches
            patches = self.image_to_patches(image)
            
            # 2. PatchåµŒå…¥
            embedded_patches = self.patch_embedding_forward(patches)
            
            # 3. æ·»åŠ class tokenå’Œä½ç½®åµŒå…¥
            sequence = self.add_class_token_and_position(embedded_patches)
            
            # 4. Transformerç¼–ç å™¨ (ç®€åŒ–ä¸ºå•å±‚)
            transformer = TransformerBlock(self.d_model, self.num_heads, self.d_model * 4)
            encoded_sequence, attention_weights = transformer.forward(sequence)
            
            # 5. æå–class tokençš„è¾“å‡ºç”¨äºåˆ†ç±»
            class_output = encoded_sequence[0]
            
            # 6. åˆ†ç±»å¤´ (ç®€åŒ–ä¸ºçº¿æ€§å±‚)
            # åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œæ˜¯ä¸€ä¸ªå…¨è¿æ¥å±‚
            classification_score = sum(class_output) / len(class_output)  # ç®€åŒ–
            
            return classification_score, attention_weights
    
    # ViTæ¼”ç¤º
    print("Vision Transformeræ¼”ç¤º:")
    
    # åˆ›å»ºViTæ¨¡å‹ (å°å°ºå¯¸ç”¨äºæ¼”ç¤º)
    vit = VisionTransformer(image_size=32, patch_size=8, d_model=64, num_heads=4)
    
    # æ¨¡æ‹Ÿè¾“å…¥å›¾åƒ
    dummy_image = "dummy_image"  # åœ¨å®é™…ä¸­æ˜¯3Dæ•°ç»„
    
    # å‰å‘ä¼ æ’­
    classification_score, attention_weights = vit.forward(dummy_image)
    
    print(f"\nåˆ†ç±»è¾“å‡º: {classification_score:.6f}")
    
    print(f"\nViTç›¸æ¯”CNNçš„ä¼˜åŠ¿:")
    print(f"â€¢ é•¿è·ç¦»ä¾èµ–ï¼šè‡ªæ³¨æ„åŠ›å¯ä»¥æ•è·å…¨å±€ä¿¡æ¯")
    print(f"â€¢ å¯è§£é‡Šæ€§ï¼šæ³¨æ„åŠ›æƒé‡æä¾›å¯è§†åŒ–")
    print(f"â€¢ å¯æ‰©å±•æ€§ï¼šå®¹æ˜“æ‰©å±•åˆ°å¤§æ¨¡å‹")
    print(f"â€¢ é¢„è®­ç»ƒï¼šå¯ä»¥åœ¨å¤§æ•°æ®é›†ä¸Šé¢„è®­ç»ƒ")

def efficient_neural_architecture():
    """é«˜æ•ˆç¥ç»ç½‘ç»œæ¶æ„"""
    print("\n" + "="*70)
    print("EfficientNetä¸ç¥ç»æ¶æ„æœç´¢")
    print("="*70)
    
    print("EfficientNetæ ¸å¿ƒåˆ›æ–°:")
    print("â€¢ å¤åˆç¼©æ”¾æ³•åˆ™ï¼šå¹³è¡¡æ·±åº¦ã€å®½åº¦ã€åˆ†è¾¨ç‡")
    print("â€¢ MBConvå—ï¼šç§»åŠ¨å€’ç½®æ®‹å·®å—")
    print("â€¢ Squeeze-and-Excitationï¼šé€šé“æ³¨æ„åŠ›")
    print("â€¢ ç¥ç»æ¶æ„æœç´¢(NAS)ï¼šè‡ªåŠ¨è®¾è®¡ç½‘ç»œ")
    print()
    
    class MBConvBlock:
        """Mobile Inverted Bottleneck Convolutionå—"""
        
        def __init__(self, input_channels, output_channels, expansion_ratio=6, 
                     kernel_size=3, stride=1, se_ratio=0.25):
            self.input_channels = input_channels
            self.output_channels = output_channels
            self.expansion_ratio = expansion_ratio
            self.kernel_size = kernel_size
            self.stride = stride
            self.se_ratio = se_ratio
            
            # è®¡ç®—ä¸­é—´é€šé“æ•°
            self.expanded_channels = input_channels * expansion_ratio
            
            # åˆå§‹åŒ–æƒé‡ (ç®€åŒ–è¡¨ç¤º)
            self.expand_conv = self._init_conv_weights(input_channels, self.expanded_channels, 1)
            self.depthwise_conv = self._init_conv_weights(self.expanded_channels, self.expanded_channels, kernel_size)
            self.se_weights = self._init_se_weights()
            self.project_conv = self._init_conv_weights(self.expanded_channels, output_channels, 1)
            
        def _init_conv_weights(self, in_ch, out_ch, kernel_size):
            """åˆå§‹åŒ–å·ç§¯æƒé‡"""
            fan_out = out_ch * kernel_size * kernel_size
            std = math.sqrt(2.0 / fan_out)
            return {
                'weight': random.gauss(0, std),
                'bias': 0.0,
                'in_channels': in_ch,
                'out_channels': out_ch,
                'kernel_size': kernel_size
            }
        
        def _init_se_weights(self):
            """åˆå§‹åŒ–Squeeze-and-Excitationæƒé‡"""
            se_channels = max(1, int(self.input_channels * self.se_ratio))
            return {
                'fc1': self._init_conv_weights(self.expanded_channels, se_channels, 1),
                'fc2': self._init_conv_weights(se_channels, self.expanded_channels, 1)
            }
        
        def squeeze_and_excitation(self, x):
            """Squeeze-and-Excitationæ¨¡å—"""
            batch_size = len(x)
            channels = len(x[0])
            
            # Global Average Pooling (Squeeze)
            se_input = []
            for b in range(batch_size):
                channel_means = []
                for c in range(channels):
                    mean_val = sum(x[b][c]) / len(x[b][c])  # ç®€åŒ–çš„GAP
                    channel_means.append(mean_val)
                se_input.append(channel_means)
            
            # Excitation: FC -> ReLU -> FC -> Sigmoid
            se_output = []
            for b in range(batch_size):
                # ç¬¬ä¸€ä¸ªFCå±‚ + ReLU
                fc1_out = []
                se_channels = self.se_weights['fc1']['out_channels']
                for i in range(se_channels):
                    val = sum(se_input[b][j] * random.uniform(-0.1, 0.1) 
                             for j in range(len(se_input[b])))
                    fc1_out.append(max(0, val))  # ReLU
                
                # ç¬¬äºŒä¸ªFCå±‚ + Sigmoid
                fc2_out = []
                for i in range(channels):
                    val = sum(fc1_out[j] * random.uniform(-0.1, 0.1) 
                             for j in range(len(fc1_out)))
                    fc2_out.append(1.0 / (1.0 + math.exp(-val)))  # Sigmoid
                
                se_output.append(fc2_out)
            
            # åº”ç”¨æ³¨æ„åŠ›æƒé‡
            attended = []
            for b in range(batch_size):
                attended_channels = []
                for c in range(channels):
                    attended_channel = [val * se_output[b][c] for val in x[b][c]]
                    attended_channels.append(attended_channel)
                attended.append(attended_channels)
            
            return attended
        
        def forward(self, x):
            """MBConvå—å‰å‘ä¼ æ’­"""
            batch_size = len(x)
            
            print(f"  MBConvå—å¤„ç†:")
            print(f"    è¾“å…¥é€šé“: {self.input_channels}")
            print(f"    æ‰©å±•åˆ°: {self.expanded_channels} (æ‰©å±•æ¯”ç‡: {self.expansion_ratio})")
            
            # 1. Expansion (å¦‚æœexpansion_ratio > 1)
            if self.expansion_ratio != 1:
                # 1x1 å·ç§¯æ‰©å±•é€šé“
                expanded = x  # ç®€åŒ–å®ç°
                print(f"    1x1æ‰©å±•å·ç§¯: {self.input_channels} -> {self.expanded_channels}")
            else:
                expanded = x
            
            # 2. Depthwise Convolution
            # æ·±åº¦å¯åˆ†ç¦»å·ç§¯ (ç®€åŒ–å®ç°)
            depthwise_out = expanded
            print(f"    æ·±åº¦å·ç§¯: kernel_size={self.kernel_size}, stride={self.stride}")
            
            # 3. Squeeze-and-Excitation
            if self.se_ratio > 0:
                se_out = self.squeeze_and_excitation(depthwise_out)
                print(f"    SEæ³¨æ„åŠ›: ratio={self.se_ratio}")
            else:
                se_out = depthwise_out
            
            # 4. Projection
            # 1x1 å·ç§¯æŠ•å½±åˆ°è¾“å‡ºé€šé“
            projected = se_out  # ç®€åŒ–å®ç°
            print(f"    1x1æŠ•å½±: {self.expanded_channels} -> {self.output_channels}")
            
            # 5. Residual Connection (å¦‚æœè¾“å…¥è¾“å‡ºå½¢çŠ¶ç›¸åŒ)
            if (self.input_channels == self.output_channels and 
                self.stride == 1 and len(x) == len(projected)):
                
                # æ®‹å·®è¿æ¥
                output = []
                for b in range(batch_size):
                    residual_channels = []
                    for c in range(min(len(x[b]), len(projected[b]))):
                        residual_channel = []
                        for i in range(min(len(x[b][c]), len(projected[b][c]))):
                            residual_channel.append(x[b][c][i] + projected[b][c][i])
                        residual_channels.append(residual_channel)
                    output.append(residual_channels)
                
                print(f"    æ®‹å·®è¿æ¥: âœ“")
            else:
                output = projected
                print(f"    æ®‹å·®è¿æ¥: âœ— (å½¢çŠ¶ä¸åŒ¹é…)")
            
            return output
    
    class CompoundScaling:
        """å¤åˆç¼©æ”¾ç­–ç•¥"""
        
        def __init__(self, phi=1.0):
            """
            phi: å¤åˆç³»æ•°
            depth_multiplier = Î±^phi
            width_multiplier = Î²^phi  
            resolution_multiplier = Î³^phi
            çº¦æŸ: Î± * Î²^2 * Î³^2 â‰ˆ 2
            """
            self.phi = phi
            self.alpha = 1.2  # æ·±åº¦ç¼©æ”¾å› å­
            self.beta = 1.1   # å®½åº¦ç¼©æ”¾å› å­
            self.gamma = 1.15 # åˆ†è¾¨ç‡ç¼©æ”¾å› å­
            
        def scale_network(self, base_depth, base_width, base_resolution):
            """æŒ‰å¤åˆç¼©æ”¾æ³•åˆ™ç¼©æ”¾ç½‘ç»œ"""
            depth_multiplier = self.alpha ** self.phi
            width_multiplier = self.beta ** self.phi
            resolution_multiplier = self.gamma ** self.phi
            
            scaled_depth = int(base_depth * depth_multiplier)
            scaled_width = int(base_width * width_multiplier)
            scaled_resolution = int(base_resolution * resolution_multiplier)
            
            # éªŒè¯çº¦æŸæ¡ä»¶
            constraint_value = depth_multiplier * (width_multiplier ** 2) * (resolution_multiplier ** 2)
            target_flops = 2 ** self.phi
            
            print(f"å¤åˆç¼©æ”¾ (Ï†={self.phi}):")
            print(f"  æ·±åº¦: {base_depth} -> {scaled_depth} (Ã—{depth_multiplier:.2f})")
            print(f"  å®½åº¦: {base_width} -> {scaled_width} (Ã—{width_multiplier:.2f})")
            print(f"  åˆ†è¾¨ç‡: {base_resolution} -> {scaled_resolution} (Ã—{resolution_multiplier:.2f})")
            print(f"  çº¦æŸæ£€æŸ¥: Î±Ã—Î²Â²Ã—Î³Â² = {constraint_value:.2f} â‰ˆ {target_flops:.2f}")
            
            return scaled_depth, scaled_width, scaled_resolution
    
    # EfficientNetæ¼”ç¤º
    print("EfficientNetæ¶æ„æ¼”ç¤º:")
    
    # 1. MBConvå—æ¼”ç¤º
    print("\n1. MBConvå—æ¼”ç¤º:")
    mbconv = MBConvBlock(input_channels=32, output_channels=64, expansion_ratio=6)
    
    # æ¨¡æ‹Ÿè¾“å…¥ (batch_size=1, channels=32, spatial_dimsç®€åŒ–)
    dummy_input = [[[random.uniform(0, 1) for _ in range(10)] for _ in range(32)]]
    
    mbconv_output = mbconv.forward(dummy_input)
    
    # 2. å¤åˆç¼©æ”¾æ¼”ç¤º
    print(f"\n2. å¤åˆç¼©æ”¾æ¼”ç¤º:")
    base_config = {
        'depth': 16,      # åŸºç¡€å±‚æ•°
        'width': 64,      # åŸºç¡€é€šé“æ•°
        'resolution': 224 # åŸºç¡€åˆ†è¾¨ç‡
    }
    
    for phi in [0, 1, 2, 3]:
        scaler = CompoundScaling(phi=phi)
        scaled_depth, scaled_width, scaled_resolution = scaler.scale_network(
            base_config['depth'], base_config['width'], base_config['resolution'])
        print()
    
    print(f"\nEfficientNetçš„ä¼˜åŠ¿:")
    print(f"â€¢ å‚æ•°æ•ˆç‡ï¼šç›¸åŒç²¾åº¦ä¸‹å‚æ•°æ›´å°‘")
    print(f"â€¢ è®¡ç®—æ•ˆç‡ï¼šFLOPsæ›´ä½")
    print(f"â€¢ ç³»ç»ŸåŒ–ç¼©æ”¾ï¼šé¿å…æ‰‹å·¥è°ƒå‚")
    print(f"â€¢ è¿ç§»æ€§å¥½ï¼šå®¹æ˜“é€‚é…ä¸åŒä»»åŠ¡")

def mixture_of_experts():
    """æ··åˆä¸“å®¶æ¨¡å‹"""
    print("\n" + "="*70)
    print("æ··åˆä¸“å®¶æ¨¡å‹ (Mixture of Experts)")
    print("="*70)
    
    print("MoEæ ¸å¿ƒæ€æƒ³:")
    print("â€¢ ç¨€ç–æ¿€æ´»ï¼šåªæ¿€æ´»éƒ¨åˆ†ä¸“å®¶")
    print("â€¢ é—¨æ§ç½‘ç»œï¼šå­¦ä¹ å¦‚ä½•é€‰æ‹©ä¸“å®¶")
    print("â€¢ ä¸“å®¶ç½‘ç»œï¼šä¸“é—¨å¤„ç†ç‰¹å®šç±»å‹çš„è¾“å…¥")
    print("â€¢ å¯æ‰©å±•æ€§ï¼šå¢åŠ ä¸“å®¶è€Œä¸å¢åŠ è®¡ç®—")
    print()
    
    class MixtureOfExperts:
        """æ··åˆä¸“å®¶æ¨¡å‹å®ç°"""
        
        def __init__(self, input_dim, output_dim, num_experts=8, top_k=2):
            self.input_dim = input_dim
            self.output_dim = output_dim
            self.num_experts = num_experts
            self.top_k = top_k
            
            # åˆå§‹åŒ–é—¨æ§ç½‘ç»œ
            self.gate_weights = self._init_weights(input_dim, num_experts)
            self.gate_bias = [0.0] * num_experts
            
            # åˆå§‹åŒ–ä¸“å®¶ç½‘ç»œ
            self.experts = []
            for i in range(num_experts):
                expert = {
                    'weights': self._init_weights(input_dim, output_dim),
                    'bias': [random.uniform(-0.1, 0.1) for _ in range(output_dim)]
                }
                self.experts.append(expert)
            
            print(f"MoEé…ç½®:")
            print(f"  ä¸“å®¶æ•°é‡: {num_experts}")
            print(f"  Top-K: {top_k}")
            print(f"  è¾“å…¥ç»´åº¦: {input_dim}")
            print(f"  è¾“å‡ºç»´åº¦: {output_dim}")
        
        def _init_weights(self, in_dim, out_dim):
            """Xavieråˆå§‹åŒ–"""
            limit = math.sqrt(6.0 / (in_dim + out_dim))
            return [[random.uniform(-limit, limit) for _ in range(out_dim)] 
                   for _ in range(in_dim)]
        
        def gating_network(self, x):
            """é—¨æ§ç½‘ç»œè®¡ç®—ä¸“å®¶æƒé‡"""
            # è®¡ç®—é—¨æ§åˆ†æ•° g = softmax(x @ W_g + b_g)
            gate_scores = []
            for i in range(self.num_experts):
                score = sum(x[j] * self.gate_weights[j][i] for j in range(len(x)))
                score += self.gate_bias[i]
                gate_scores.append(score)
            
            # Softmaxå½’ä¸€åŒ–
            max_score = max(gate_scores)
            exp_scores = [math.exp(s - max_score) for s in gate_scores]
            sum_exp = sum(exp_scores)
            gate_weights = [exp_s / sum_exp for exp_s in exp_scores]
            
            return gate_weights
        
        def select_top_k_experts(self, gate_weights):
            """é€‰æ‹©Top-Kä¸“å®¶"""
            # è·å–æƒé‡å’Œç´¢å¼•çš„é…å¯¹
            expert_pairs = [(weight, idx) for idx, weight in enumerate(gate_weights)]
            
            # æŒ‰æƒé‡é™åºæ’åº
            expert_pairs.sort(reverse=True)
            
            # é€‰æ‹©Top-K
            top_k_experts = expert_pairs[:self.top_k]
            
            # é‡æ–°å½’ä¸€åŒ–Top-Kæƒé‡
            top_k_weights = [pair[0] for pair in top_k_experts]
            weight_sum = sum(top_k_weights)
            
            if weight_sum > 0:
                normalized_weights = [w / weight_sum for w in top_k_weights]
            else:
                normalized_weights = [1.0 / self.top_k] * self.top_k
            
            # è¿”å›é€‰ä¸­çš„ä¸“å®¶ç´¢å¼•å’Œå½’ä¸€åŒ–æƒé‡
            selected_experts = [(pair[1], normalized_weights[i]) 
                              for i, pair in enumerate(top_k_experts)]
            
            return selected_experts
        
        def expert_forward(self, x, expert_idx):
            """å•ä¸ªä¸“å®¶çš„å‰å‘ä¼ æ’­"""
            expert = self.experts[expert_idx]
            
            # çº¿æ€§å˜æ¢: y = x @ W + b
            output = []
            for i in range(self.output_dim):
                value = sum(x[j] * expert['weights'][j][i] for j in range(len(x)))
                value += expert['bias'][i]
                output.append(value)
            
            return output
        
        def forward(self, x):
            """MoEå‰å‘ä¼ æ’­"""
            # 1. è®¡ç®—é—¨æ§æƒé‡
            gate_weights = self.gating_network(x)
            
            # 2. é€‰æ‹©Top-Kä¸“å®¶
            selected_experts = self.select_top_k_experts(gate_weights)
            
            # 3. è®¡ç®—é€‰ä¸­ä¸“å®¶çš„è¾“å‡º
            expert_outputs = []
            expert_info = []
            
            for expert_idx, weight in selected_experts:
                output = self.expert_forward(x, expert_idx)
                expert_outputs.append(output)
                expert_info.append((expert_idx, weight))
            
            # 4. åŠ æƒèåˆä¸“å®¶è¾“å‡º
            final_output = [0.0] * self.output_dim
            
            for i, (output, (expert_idx, weight)) in enumerate(zip(expert_outputs, expert_info)):
                for j in range(self.output_dim):
                    final_output[j] += weight * output[j]
            
            return final_output, expert_info, gate_weights
        
        def compute_load_balancing_loss(self, gate_weights_batch):
            """è®¡ç®—è´Ÿè½½å‡è¡¡æŸå¤±"""
            # è®¡ç®—æ¯ä¸ªä¸“å®¶çš„å¹³å‡é—¨æ§æƒé‡
            avg_gate_weights = [0.0] * self.num_experts
            batch_size = len(gate_weights_batch)
            
            for gate_weights in gate_weights_batch:
                for i in range(self.num_experts):
                    avg_gate_weights[i] += gate_weights[i] / batch_size
            
            # ç†æƒ³æƒ…å†µä¸‹æ¯ä¸ªä¸“å®¶åº”è¯¥æœ‰ç›¸ç­‰çš„æƒé‡
            target_weight = 1.0 / self.num_experts
            
            # è®¡ç®—è´Ÿè½½ä¸å‡è¡¡åº¦
            imbalance = sum((w - target_weight) ** 2 for w in avg_gate_weights)
            
            return imbalance, avg_gate_weights
    
    # MoEæ¼”ç¤º
    print("æ··åˆä¸“å®¶æ¨¡å‹æ¼”ç¤º:")
    
    # åˆ›å»ºMoEæ¨¡å‹
    moe = MixtureOfExperts(input_dim=10, output_dim=5, num_experts=4, top_k=2)
    
    # æ¨¡æ‹Ÿä¸€æ‰¹è¾“å…¥
    batch_inputs = []
    batch_gate_weights = []
    
    for i in range(3):
        # ç”Ÿæˆä¸åŒç±»å‹çš„è¾“å…¥
        if i == 0:
            # ç±»å‹1ï¼šå‰åŠéƒ¨åˆ†éé›¶
            x = [random.uniform(0.5, 1.0) for _ in range(5)] + [0.0] * 5
        elif i == 1:
            # ç±»å‹2ï¼šååŠéƒ¨åˆ†éé›¶
            x = [0.0] * 5 + [random.uniform(0.5, 1.0) for _ in range(5)]
        else:
            # ç±»å‹3ï¼šéšæœºåˆ†å¸ƒ
            x = [random.uniform(-0.5, 0.5) for _ in range(10)]
        
        batch_inputs.append(x)
    
    print(f"\nå¤„ç†{len(batch_inputs)}ä¸ªè¾“å…¥æ ·æœ¬:")
    
    for i, x in enumerate(batch_inputs):
        output, expert_info, gate_weights = moe.forward(x)
        batch_gate_weights.append(gate_weights)
        
        print(f"\næ ·æœ¬{i+1}:")
        print(f"  è¾“å…¥ç‰¹å¾: [{', '.join(f'{v:.2f}' for v in x[:5])}...{', '.join(f'{v:.2f}' for v in x[-5:])}]")
        print(f"  é—¨æ§æƒé‡: [{', '.join(f'{w:.3f}' for w in gate_weights)}]")
        print(f"  é€‰ä¸­ä¸“å®¶: {[(idx, f'{w:.3f}') for idx, w in expert_info]}")
        print(f"  è¾“å‡º: [{', '.join(f'{v:.3f}' for v in output)}]")
    
    # è®¡ç®—è´Ÿè½½å‡è¡¡
    imbalance, avg_weights = moe.compute_load_balancing_loss(batch_gate_weights)
    
    print(f"\nè´Ÿè½½å‡è¡¡åˆ†æ:")
    print(f"  æ¯ä¸ªä¸“å®¶çš„å¹³å‡æƒé‡: [{', '.join(f'{w:.3f}' for w in avg_weights)}]")
    print(f"  ç†æƒ³æƒé‡: {1.0/moe.num_experts:.3f}")
    print(f"  è´Ÿè½½ä¸å‡è¡¡åº¦: {imbalance:.6f}")
    
    print(f"\nMoEçš„ä¼˜åŠ¿:")
    print(f"â€¢ æ¨¡å‹å®¹é‡ï¼šå¢åŠ ä¸“å®¶æ•°é‡è€Œä¸å¢åŠ è®¡ç®—")
    print(f"â€¢ ä¸“ä¸šåŒ–ï¼šä¸åŒä¸“å®¶å­¦ä¹ ä¸åŒæ¨¡å¼")  
    print(f"â€¢ æ•ˆç‡ï¼šç¨€ç–æ¿€æ´»ï¼Œåªä½¿ç”¨Top-Kä¸“å®¶")
    print(f"â€¢ å¯æ‰©å±•ï¼šå®¹æ˜“æ‰©å±•åˆ°å¤§è§„æ¨¡æ¨¡å‹")

def main():
    """ä¸»å‡½æ•°"""
    print("å‰æ²¿æ·±åº¦å­¦ä¹ æ¶æ„")
    print("=" * 70)
    
    cutting_edge_intro()
    transformer_detailed_implementation()
    vision_transformer_implementation()
    efficient_neural_architecture()
    mixture_of_experts()
    
    print("\n" + "=" * 70)
    print("ğŸ¯ å‰æ²¿æ¶æ„æ€»ç»“")
    print()
    print("æŒæ¡çš„å‰æ²¿æŠ€æœ¯:")
    print("â€¢ Transformerï¼šè‡ªæ³¨æ„åŠ›æœºåˆ¶çš„å®Œæ•´å®ç°")
    print("â€¢ Vision Transformerï¼šå›¾åƒé¢†åŸŸçš„Transformeråº”ç”¨")
    print("â€¢ EfficientNetï¼šå¤åˆç¼©æ”¾ä¸é«˜æ•ˆæ¶æ„è®¾è®¡")
    print("â€¢ æ··åˆä¸“å®¶ï¼šç¨€ç–æ¿€æ´»çš„å¤§è§„æ¨¡æ¨¡å‹")
    print()
    print("è¿™äº›å‰æ²¿æ¶æ„ä»£è¡¨äº†æ·±åº¦å­¦ä¹ çš„æœ€æ–°å‘å±•æ–¹å‘ï¼Œ")
    print("ç†è§£å¹¶æŒæ¡å®ƒä»¬å°†å¸®åŠ©ä½ è·Ÿä¸ŠæŠ€æœ¯å‰æ²¿ï¼Œ")
    print("ä¸ºç ”ç©¶å’Œåº”ç”¨æä¾›å¼ºå¤§çš„å·¥å…·ï¼")

if __name__ == "__main__":
    main()