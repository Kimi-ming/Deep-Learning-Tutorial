# -*- coding: utf-8 -*-
# æ·±åº¦å­¦ä¹ æ•°å­¦åŸç†æ·±åº¦è§£æ
# Mathematical Foundations of Deep Learning: æ·±å…¥ç†è§£æ•°å­¦åŸºç¡€

import random
import math
import numpy as np

def mathematical_foundations_intro():
    """æ•°å­¦åŸºç¡€ä»‹ç»"""
    print("=== æ·±åº¦å­¦ä¹ æ•°å­¦åŸç†æ·±åº¦è§£æ ===")
    print("æ·±å…¥ç†è§£æ·±åº¦å­¦ä¹ èƒŒåçš„æ•°å­¦åŸç†")
    print()
    print("æ ¸å¿ƒæ•°å­¦æ¦‚å¿µ:")
    print("â€¢ å¤šå…ƒå¾®ç§¯åˆ†ä¸é“¾å¼æ³•åˆ™")
    print("â€¢ çº¿æ€§ä»£æ•°ä¸çŸ©é˜µè¿ç®—")
    print("â€¢ æ¦‚ç‡è®ºä¸ä¿¡æ¯è®º")
    print("â€¢ ä¼˜åŒ–ç†è®ºä¸å‡¸ä¼˜åŒ–")
    print("â€¢ æ³›å‡½åˆ†æä¸å˜åˆ†æ³•")
    print()

def chain_rule_detailed_analysis():
    """é“¾å¼æ³•åˆ™è¯¦ç»†åˆ†æ"""
    print("\n" + "="*60)
    print("é“¾å¼æ³•åˆ™ä¸åå‘ä¼ æ’­çš„æ•°å­¦åŸç†")
    print("="*60)
    
    print("è®¾ç¥ç»ç½‘ç»œçš„å¤åˆå‡½æ•°ä¸º: y = f(g(h(x)))")
    print("å…¶ä¸­æ¯ä¸€å±‚éƒ½æ˜¯å‰ä¸€å±‚çš„å‡½æ•°")
    print()
    
    print("1. å•å˜é‡é“¾å¼æ³•åˆ™:")
    print("   dy/dx = (dy/dg) Ã— (dg/dh) Ã— (dh/dx)")
    print()
    
    print("2. å¤šå˜é‡é“¾å¼æ³•åˆ™:")
    print("   å¯¹äº z = f(x,y), x = g(t), y = h(t)")
    print("   dz/dt = (âˆ‚z/âˆ‚x)(dx/dt) + (âˆ‚z/âˆ‚y)(dy/dt)")
    print()
    
    class ChainRuleDemo:
        """é“¾å¼æ³•åˆ™æ¼”ç¤º"""
        
        def __init__(self):
            self.computation_graph = {}
            self.gradients = {}
            
        def forward_pass(self, x):
            """å‰å‘ä¼ æ’­æ¼”ç¤º"""
            # æ„å»ºè®¡ç®—å›¾: y = sin(x^2 + 3x)
            a = x * x  # a = x^2
            b = 3 * x  # b = 3x  
            c = a + b  # c = x^2 + 3x
            y = math.sin(c)  # y = sin(c)
            
            # ä¿å­˜ä¸­é—´ç»“æœç”¨äºåå‘ä¼ æ’­
            self.computation_graph = {
                'x': x, 'a': a, 'b': b, 'c': c, 'y': y
            }
            
            return y
            
        def backward_pass(self):
            """åå‘ä¼ æ’­æ¼”ç¤º"""
            x = self.computation_graph['x']
            c = self.computation_graph['c']
            
            # åå‘è®¡ç®—æ¢¯åº¦
            # dy/dy = 1
            dy_dy = 1
            
            # dy/dc = cos(c)  
            dy_dc = math.cos(c)
            
            # dc/da = 1, dc/db = 1
            dc_da = 1
            dc_db = 1
            
            # da/dx = 2x, db/dx = 3
            da_dx = 2 * x
            db_dx = 3
            
            # åº”ç”¨é“¾å¼æ³•åˆ™: dy/dx = (dy/dc) Ã— [(dc/da)(da/dx) + (dc/db)(db/dx)]
            dy_dx = dy_dc * (dc_da * da_dx + dc_db * db_dx)
            
            self.gradients = {
                'dy/dc': dy_dc,
                'dc/da': dc_da,
                'dc/db': dc_db, 
                'da/dx': da_dx,
                'db/dx': db_dx,
                'dy/dx': dy_dx
            }
            
            return dy_dx
            
        def analytical_gradient(self, x):
            """è§£ææ¢¯åº¦è®¡ç®—ï¼ˆç”¨äºéªŒè¯ï¼‰"""
            # y = sin(x^2 + 3x)
            # dy/dx = cos(x^2 + 3x) Ã— (2x + 3)
            return math.cos(x*x + 3*x) * (2*x + 3)
    
    # æ¼”ç¤ºé“¾å¼æ³•åˆ™è®¡ç®—
    demo = ChainRuleDemo()
    x_val = 2.0
    
    print(f"æ¼”ç¤ºè®¡ç®—: y = sin(xÂ² + 3x), x = {x_val}")
    
    # å‰å‘ä¼ æ’­
    y_val = demo.forward_pass(x_val)
    print(f"å‰å‘ä¼ æ’­ç»“æœ: y = {y_val:.6f}")
    
    # åå‘ä¼ æ’­
    grad_numerical = demo.backward_pass()
    grad_analytical = demo.analytical_gradient(x_val)
    
    print(f"\næ¢¯åº¦è®¡ç®—è¿‡ç¨‹:")
    for name, value in demo.gradients.items():
        print(f"  {name} = {value:.6f}")
    
    print(f"\næ¢¯åº¦éªŒè¯:")
    print(f"  æ•°å€¼è®¡ç®—: dy/dx = {grad_numerical:.6f}")
    print(f"  è§£æè®¡ç®—: dy/dx = {grad_analytical:.6f}")
    print(f"  è¯¯å·®: {abs(grad_numerical - grad_analytical):.10f}")

def information_theory_in_deep_learning():
    """ä¿¡æ¯è®ºåœ¨æ·±åº¦å­¦ä¹ ä¸­çš„åº”ç”¨"""
    print("\n" + "="*60)
    print("ä¿¡æ¯è®ºä¸æ·±åº¦å­¦ä¹ ")
    print("="*60)
    
    print("æ ¸å¿ƒæ¦‚å¿µ:")
    print("â€¢ ç†µ(Entropy): ä¿¡æ¯çš„åº¦é‡")
    print("â€¢ äº¤å‰ç†µ(Cross-Entropy): æŸå¤±å‡½æ•°çš„ç†è®ºåŸºç¡€")
    print("â€¢ KLæ•£åº¦: åˆ†å¸ƒä¹‹é—´çš„è·ç¦»")
    print("â€¢ äº’ä¿¡æ¯: å˜é‡é—´çš„ä¾èµ–å…³ç³»")
    print()
    
    class InformationTheory:
        """ä¿¡æ¯è®ºè®¡ç®—å·¥å…·"""
        
        @staticmethod
        def entropy(probabilities):
            """è®¡ç®—ç†µ H(X) = -Î£ p(x) log p(x)"""
            entropy = 0
            for p in probabilities:
                if p > 0:  # é¿å…log(0)
                    entropy -= p * math.log2(p)
            return entropy
            
        @staticmethod
        def cross_entropy(true_dist, pred_dist):
            """è®¡ç®—äº¤å‰ç†µ H(P,Q) = -Î£ p(x) log q(x)"""
            cross_ent = 0
            for p, q in zip(true_dist, pred_dist):
                if p > 0 and q > 0:
                    cross_ent -= p * math.log2(q)
            return cross_ent
            
        @staticmethod
        def kl_divergence(true_dist, pred_dist):
            """è®¡ç®—KLæ•£åº¦ D_KL(P||Q) = Î£ p(x) log(p(x)/q(x))"""
            kl_div = 0
            for p, q in zip(true_dist, pred_dist):
                if p > 0 and q > 0:
                    kl_div += p * math.log2(p / q)
            return kl_div
            
        @staticmethod
        def mutual_information(joint_prob, marginal_x, marginal_y):
            """è®¡ç®—äº’ä¿¡æ¯ I(X;Y) = Î£ p(x,y) log(p(x,y)/(p(x)p(y)))"""
            mi = 0
            for i in range(len(joint_prob)):
                for j in range(len(joint_prob[0])):
                    p_xy = joint_prob[i][j]
                    p_x = marginal_x[i]
                    p_y = marginal_y[j]
                    if p_xy > 0 and p_x > 0 and p_y > 0:
                        mi += p_xy * math.log2(p_xy / (p_x * p_y))
            return mi
    
    # ä¿¡æ¯è®ºåº”ç”¨æ¼”ç¤º
    print("ä¿¡æ¯è®ºè®¡ç®—ç¤ºä¾‹:")
    
    # 1. ç†µçš„è®¡ç®—
    uniform_dist = [0.25, 0.25, 0.25, 0.25]  # å‡åŒ€åˆ†å¸ƒ
    skewed_dist = [0.7, 0.2, 0.05, 0.05]     # åæ–œåˆ†å¸ƒ
    
    entropy_uniform = InformationTheory.entropy(uniform_dist)
    entropy_skewed = InformationTheory.entropy(skewed_dist)
    
    print(f"\n1. ç†µè®¡ç®—:")
    print(f"   å‡åŒ€åˆ†å¸ƒ {uniform_dist}: H = {entropy_uniform:.4f} bits")
    print(f"   åæ–œåˆ†å¸ƒ {skewed_dist}: H = {entropy_skewed:.4f} bits")
    print(f"   ç»“è®º: å‡åŒ€åˆ†å¸ƒå…·æœ‰æœ€å¤§ç†µ")
    
    # 2. äº¤å‰ç†µä¸KLæ•£åº¦
    true_dist = [0.6, 0.3, 0.1]
    pred_dist = [0.5, 0.4, 0.1]
    
    cross_ent = InformationTheory.cross_entropy(true_dist, pred_dist)
    entropy_true = InformationTheory.entropy(true_dist)
    kl_div = InformationTheory.kl_divergence(true_dist, pred_dist)
    
    print(f"\n2. äº¤å‰ç†µä¸KLæ•£åº¦:")
    print(f"   çœŸå®åˆ†å¸ƒ: {true_dist}")
    print(f"   é¢„æµ‹åˆ†å¸ƒ: {pred_dist}")
    print(f"   H(P): {entropy_true:.4f}")
    print(f"   H(P,Q): {cross_ent:.4f}")
    print(f"   D_KL(P||Q): {kl_div:.4f}")
    print(f"   éªŒè¯: H(P,Q) = H(P) + D_KL(P||Q) = {entropy_true + kl_div:.4f}")
    
    print(f"\nåœ¨æ·±åº¦å­¦ä¹ ä¸­çš„åº”ç”¨:")
    print(f"â€¢ äº¤å‰ç†µæŸå¤±å‡½æ•°è¡¡é‡é¢„æµ‹åˆ†å¸ƒä¸çœŸå®åˆ†å¸ƒçš„å·®å¼‚")
    print(f"â€¢ KLæ•£åº¦ç”¨äºå˜åˆ†æ¨æ–­å’Œæ­£åˆ™åŒ–")
    print(f"â€¢ äº’ä¿¡æ¯ç”¨äºç‰¹å¾é€‰æ‹©å’Œè¡¨ç¤ºå­¦ä¹ ")

def optimization_theory_advanced():
    """ä¼˜åŒ–ç†è®ºé«˜çº§å†…å®¹"""
    print("\n" + "="*60)
    print("ä¼˜åŒ–ç†è®ºä¸æ·±åº¦å­¦ä¹ ")
    print("="*60)
    
    print("é«˜çº§ä¼˜åŒ–æ¦‚å¿µ:")
    print("â€¢ å‡¸ä¼˜åŒ–ä¸éå‡¸ä¼˜åŒ–")
    print("â€¢ éç‚¹é—®é¢˜ä¸é€ƒé€¸ç­–ç•¥")
    print("â€¢ äºŒé˜¶ä¼˜åŒ–æ–¹æ³•")
    print("â€¢ è‡ªé€‚åº”å­¦ä¹ ç‡æ–¹æ³•")
    print()
    
    class AdvancedOptimizers:
        """é«˜çº§ä¼˜åŒ–å™¨å®ç°"""
        
        def __init__(self, params_shape):
            self.params_shape = params_shape
            self.reset()
            
        def reset(self):
            """é‡ç½®ä¼˜åŒ–å™¨çŠ¶æ€"""
            self.m = [0.0] * self.params_shape  # ä¸€é˜¶åŠ¨é‡
            self.v = [0.0] * self.params_shape  # äºŒé˜¶åŠ¨é‡
            self.t = 0  # æ—¶é—´æ­¥
            
        def adam_optimizer(self, gradients, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):
            """Adamä¼˜åŒ–å™¨è¯¦ç»†å®ç°"""
            self.t += 1
            
            updated_params = []
            
            for i in range(len(gradients)):
                # æ›´æ–°åç½®ä¿®æ­£çš„ä¸€é˜¶å’ŒäºŒé˜¶åŠ¨é‡ä¼°è®¡
                self.m[i] = beta1 * self.m[i] + (1 - beta1) * gradients[i]
                self.v[i] = beta2 * self.v[i] + (1 - beta2) * (gradients[i] ** 2)
                
                # åç½®ä¿®æ­£
                m_hat = self.m[i] / (1 - beta1 ** self.t)
                v_hat = self.v[i] / (1 - beta2 ** self.t)
                
                # å‚æ•°æ›´æ–°
                update = learning_rate * m_hat / (math.sqrt(v_hat) + epsilon)
                updated_params.append(update)
            
            return updated_params
            
        def rmsprop_optimizer(self, gradients, learning_rate=0.001, beta=0.9, epsilon=1e-8):
            """RMSpropä¼˜åŒ–å™¨å®ç°"""
            updated_params = []
            
            for i in range(len(gradients)):
                # æ›´æ–°äºŒé˜¶åŠ¨é‡
                self.v[i] = beta * self.v[i] + (1 - beta) * (gradients[i] ** 2)
                
                # å‚æ•°æ›´æ–°
                update = learning_rate * gradients[i] / (math.sqrt(self.v[i]) + epsilon)
                updated_params.append(update)
                
            return updated_params
            
        def adagrad_optimizer(self, gradients, learning_rate=0.01, epsilon=1e-8):
            """Adagradä¼˜åŒ–å™¨å®ç°"""
            updated_params = []
            
            for i in range(len(gradients)):
                # ç´¯ç§¯å¹³æ–¹æ¢¯åº¦
                self.v[i] += gradients[i] ** 2
                
                # å‚æ•°æ›´æ–°
                update = learning_rate * gradients[i] / (math.sqrt(self.v[i]) + epsilon)
                updated_params.append(update)
                
            return updated_params
    
    # ä¼˜åŒ–å™¨æ€§èƒ½æ¯”è¾ƒ
    print("ä¼˜åŒ–å™¨æ€§èƒ½æ¯”è¾ƒ:")
    
    def rosenbrock_function(x, y):
        """Rosenbrockå‡½æ•°: f(x,y) = (a-x)Â² + b(y-xÂ²)Â²"""
        a, b = 1, 100
        return (a - x)**2 + b * (y - x**2)**2
    
    def rosenbrock_gradient(x, y):
        """Rosenbrockå‡½æ•°çš„æ¢¯åº¦"""
        a, b = 1, 100
        dx = -2*(a - x) - 4*b*x*(y - x**2)
        dy = 2*b*(y - x**2)
        return [dx, dy]
    
    # æµ‹è¯•ä¸åŒä¼˜åŒ–å™¨
    optimizers = {
        'Adam': lambda opt, grad: opt.adam_optimizer(grad, learning_rate=0.01),
        'RMSprop': lambda opt, grad: opt.rmsprop_optimizer(grad, learning_rate=0.01),
        'Adagrad': lambda opt, grad: opt.adagrad_optimizer(grad, learning_rate=0.1)
    }
    
    initial_point = [-1.0, 1.0]
    target_point = [1.0, 1.0]  # å…¨å±€æœ€ä¼˜ç‚¹
    
    print(f"æµ‹è¯•å‡½æ•°: Rosenbrockå‡½æ•°")
    print(f"èµ·å§‹ç‚¹: {initial_point}")
    print(f"ç›®æ ‡ç‚¹: {target_point}")
    print(f"è¿­ä»£æ¬¡æ•°: 100")
    print()
    
    for name, optimizer_func in optimizers.items():
        opt = AdvancedOptimizers(2)
        x, y = initial_point[:]
        
        for iteration in range(100):
            grad = rosenbrock_gradient(x, y)
            updates = optimizer_func(opt, grad)
            x -= updates[0]
            y -= updates[1]
        
        final_value = rosenbrock_function(x, y)
        distance_to_optimum = math.sqrt((x - 1)**2 + (y - 1)**2)
        
        print(f"{name:>8}: æœ€ç»ˆç‚¹({x:.4f}, {y:.4f}), å‡½æ•°å€¼={final_value:.6f}, è·ç¦»æœ€ä¼˜={distance_to_optimum:.6f}")

def variational_inference_theory():
    """å˜åˆ†æ¨æ–­ç†è®º"""
    print("\n" + "="*60)
    print("å˜åˆ†æ¨æ–­ä¸æ·±åº¦ç”Ÿæˆæ¨¡å‹")
    print("="*60)
    
    print("å˜åˆ†æ¨æ–­æ ¸å¿ƒæ€æƒ³:")
    print("â€¢ ç”¨ç®€å•åˆ†å¸ƒè¿‘ä¼¼å¤æ‚çš„åéªŒåˆ†å¸ƒ")
    print("â€¢ æœ€å°åŒ–KLæ•£åº¦æ‰¾åˆ°æœ€ä½³è¿‘ä¼¼")
    print("â€¢ ELBO(Evidence Lower Bound)ä¼˜åŒ–")
    print("â€¢ é‡å‚æ•°åŒ–æŠ€å·§å®ç°æ¢¯åº¦ä¼ æ’­")
    print()
    
    class VariationalInference:
        """å˜åˆ†æ¨æ–­å®ç°"""
        
        def __init__(self, latent_dim=2):
            self.latent_dim = latent_dim
            
        def gaussian_kl_divergence(self, mu1, sigma1, mu2=0, sigma2=1):
            """è®¡ç®—ä¸¤ä¸ªé«˜æ–¯åˆ†å¸ƒçš„KLæ•£åº¦"""
            # KL(N(Î¼â‚,Ïƒâ‚Â²) || N(Î¼â‚‚,Ïƒâ‚‚Â²))
            kl = math.log(sigma2/sigma1) + (sigma1**2 + (mu1-mu2)**2)/(2*sigma2**2) - 0.5
            return kl
            
        def elbo_calculation(self, data, mu_encoder, sigma_encoder, reconstruction_loss):
            """è®¡ç®—ELBO (Evidence Lower BOund)"""
            # ELBO = E[log p(x|z)] - KL(q(z|x) || p(z))
            
            # é‡æ„æŸå¤±é¡¹ (è´Ÿå¯¹æ•°ä¼¼ç„¶)
            reconstruction_term = -reconstruction_loss
            
            # KLæ•£åº¦é¡¹ (æ­£åˆ™åŒ–é¡¹)
            kl_term = 0
            for i in range(len(mu_encoder)):
                kl_term += self.gaussian_kl_divergence(mu_encoder[i], sigma_encoder[i])
            
            elbo = reconstruction_term - kl_term
            return elbo, reconstruction_term, kl_term
            
        def reparameterization_trick(self, mu, sigma):
            """é‡å‚æ•°åŒ–æŠ€å·§"""
            # z = Î¼ + Ïƒ * Îµ, where Îµ ~ N(0,1)
            epsilon = random.gauss(0, 1)
            z = mu + sigma * epsilon
            return z, epsilon
    
    # å˜åˆ†æ¨æ–­æ¼”ç¤º
    print("å˜åˆ†è‡ªç¼–ç å™¨(VAE)åŸç†æ¼”ç¤º:")
    
    vi = VariationalInference()
    
    # æ¨¡æ‹Ÿç¼–ç å™¨è¾“å‡º
    mu_encoder = [0.5, -0.3]  # å‡å€¼
    sigma_encoder = [0.8, 1.2]  # æ ‡å‡†å·®
    reconstruction_loss = 2.5
    
    print(f"ç¼–ç å™¨è¾“å‡º:")
    print(f"  Î¼ = {mu_encoder}")
    print(f"  Ïƒ = {sigma_encoder}")
    print(f"é‡æ„æŸå¤± = {reconstruction_loss}")
    
    # è®¡ç®—ELBO
    elbo, recon_term, kl_term = vi.elbo_calculation(
        None, mu_encoder, sigma_encoder, reconstruction_loss
    )
    
    print(f"\nELBOè®¡ç®—:")
    print(f"  é‡æ„é¡¹: {recon_term:.4f}")
    print(f"  KLæ•£åº¦é¡¹: {kl_term:.4f}")
    print(f"  ELBO: {elbo:.4f}")
    
    # é‡å‚æ•°åŒ–æŠ€å·§æ¼”ç¤º
    print(f"\né‡å‚æ•°åŒ–æŠ€å·§:")
    for i, (mu, sigma) in enumerate(zip(mu_encoder, sigma_encoder)):
        z, epsilon = vi.reparameterization_trick(mu, sigma)
        print(f"  ç»´åº¦{i+1}: z = {mu:.2f} + {sigma:.2f} Ã— {epsilon:.3f} = {z:.4f}")
    
    print(f"\nå˜åˆ†æ¨æ–­çš„ä¼˜åŠ¿:")
    print(f"â€¢ æä¾›äº†å¤„ç†ä¸ç¡®å®šæ€§çš„æ¡†æ¶")
    print(f"â€¢ ä½¿å¾—ç”Ÿæˆæ¨¡å‹è®­ç»ƒæˆä¸ºå¯èƒ½")
    print(f"â€¢ æ”¯æŒåŠç›‘ç£å­¦ä¹ å’Œè¡¨ç¤ºå­¦ä¹ ")

def manifold_learning_theory():
    """æµå½¢å­¦ä¹ ç†è®º"""
    print("\n" + "="*60)
    print("æµå½¢å­¦ä¹ ä¸æ·±åº¦å­¦ä¹ ")
    print("="*60)
    
    print("æµå½¢å‡è®¾:")
    print("â€¢ é«˜ç»´æ•°æ®å¾€å¾€åˆ†å¸ƒåœ¨ä½ç»´æµå½¢ä¸Š")
    print("â€¢ æ·±åº¦ç½‘ç»œå­¦ä¹ æµå½¢çš„è¡¨ç¤º")
    print("â€¢ è‡ªç¼–ç å™¨å‘ç°æ•°æ®çš„å†…åœ¨ç»´åº¦")
    print()
    
    class ManifoldLearning:
        """æµå½¢å­¦ä¹ ç®—æ³•"""
        
        def __init__(self, input_dim, manifold_dim):
            self.input_dim = input_dim
            self.manifold_dim = manifold_dim
            
        def local_linear_embedding(self, data_points, k_neighbors=3):
            """å±€éƒ¨çº¿æ€§åµŒå…¥(LLE)ç®—æ³•çš„æ¦‚å¿µæ¼”ç¤º"""
            print("å±€éƒ¨çº¿æ€§åµŒå…¥(LLE)æ­¥éª¤:")
            print("1. æ‰¾åˆ°æ¯ä¸ªç‚¹çš„kä¸ªæœ€è¿‘é‚»")
            print("2. è®¡ç®—é‡æ„æƒé‡")
            print("3. åœ¨ä½ç»´ç©ºé—´ä¸­ä¿æŒç›¸åŒçš„æƒé‡å…³ç³»")
            
            n_points = len(data_points)
            weights = []
            
            # ç®€åŒ–æ¼”ç¤ºï¼šå‡è®¾å·²çŸ¥é‚»å±…
            for i in range(n_points):
                # æ¨¡æ‹Ÿæƒé‡è®¡ç®—
                w = [random.uniform(0, 1) for _ in range(k_neighbors)]
                w_sum = sum(w)
                w = [wi/w_sum for wi in w]  # å½’ä¸€åŒ–
                weights.append(w)
                
                print(f"   ç‚¹{i+1}çš„é‡æ„æƒé‡: {[f'{wi:.3f}' for wi in w]}")
            
            return weights
            
        def isometric_feature_mapping(self, distance_matrix):
            """ç­‰è·æ˜ å°„(Isomap)çš„æ¦‚å¿µæ¼”ç¤º"""
            print("\nç­‰è·æ˜ å°„(Isomap)æ­¥éª¤:")
            print("1. æ„å»ºkè¿‘é‚»å›¾")
            print("2. è®¡ç®—æ‰€æœ‰ç‚¹å¯¹çš„æµ‹åœ°è·ç¦»")
            print("3. åº”ç”¨å¤šç»´å°ºåº¦å˜æ¢(MDS)")
            
            n_points = len(distance_matrix)
            
            # æ¨¡æ‹Ÿæµ‹åœ°è·ç¦»è®¡ç®—
            print("æµ‹åœ°è·ç¦»çŸ©é˜µ:")
            for i in range(n_points):
                row = []
                for j in range(n_points):
                    if i == j:
                        geodesic_dist = 0.0
                    else:
                        geodesic_dist = distance_matrix[i][j] * random.uniform(1.0, 2.0)
                    row.append(geodesic_dist)
                print(f"   {[f'{d:.2f}' for d in row]}")
            
            return distance_matrix
    
    # æµå½¢å­¦ä¹ æ¼”ç¤º
    print("æµå½¢å­¦ä¹ ç®—æ³•æ¼”ç¤º:")
    
    # ç”Ÿæˆæ¨¡æ‹Ÿé«˜ç»´æ•°æ®ç‚¹
    data_points = [
        [1.0, 2.0, 0.5, 1.5],
        [1.2, 1.8, 0.6, 1.4], 
        [2.0, 1.0, 1.2, 0.8],
        [1.8, 1.2, 1.1, 0.9]
    ]
    
    distance_matrix = [
        [0.0, 0.5, 1.8, 1.5],
        [0.5, 0.0, 1.6, 1.3],
        [1.8, 1.6, 0.0, 0.3],
        [1.5, 1.3, 0.3, 0.0]
    ]
    
    ml = ManifoldLearning(input_dim=4, manifold_dim=2)
    
    print(f"è¾“å…¥æ•°æ®ç»´åº¦: {ml.input_dim}")
    print(f"æµå½¢ç»´åº¦: {ml.manifold_dim}")
    print(f"æ•°æ®ç‚¹æ•°é‡: {len(data_points)}")
    
    # LLEæ¼”ç¤º
    weights = ml.local_linear_embedding(data_points)
    
    # Isomapæ¼”ç¤º  
    geodesic_distances = ml.isometric_feature_mapping(distance_matrix)
    
    print(f"\næµå½¢å­¦ä¹ åœ¨æ·±åº¦å­¦ä¹ ä¸­çš„åº”ç”¨:")
    print(f"â€¢ è‡ªç¼–ç å™¨: å­¦ä¹ æ•°æ®çš„ç´§è‡´è¡¨ç¤º")
    print(f"â€¢ t-SNE: é«˜ç»´æ•°æ®å¯è§†åŒ–")
    print(f"â€¢ ç”Ÿæˆæ¨¡å‹: åœ¨æµå½¢ä¸Šç”Ÿæˆæ–°æ•°æ®")

def advanced_regularization_theory():
    """é«˜çº§æ­£åˆ™åŒ–ç†è®º"""
    print("\n" + "="*60)
    print("é«˜çº§æ­£åˆ™åŒ–æŠ€æœ¯")
    print("="*60)
    
    print("æ­£åˆ™åŒ–çš„ç†è®ºåŸºç¡€:")
    print("â€¢ è´å¶æ–¯è§‚ç‚¹: æ­£åˆ™åŒ–ç­‰ä»·äºå…ˆéªŒåˆ†å¸ƒ")
    print("â€¢ ä¿¡æ¯è®ºè§‚ç‚¹: æœ€å°æè¿°é•¿åº¦åŸç†")
    print("â€¢ å‡ ä½•è§‚ç‚¹: çº¦æŸä¼˜åŒ–é—®é¢˜")
    print()
    
    class AdvancedRegularization:
        """é«˜çº§æ­£åˆ™åŒ–æŠ€æœ¯å®ç°"""
        
        def spectral_normalization(self, weight_matrix, n_iterations=1):
            """è°±å½’ä¸€åŒ–"""
            print("è°±å½’ä¸€åŒ–åŸç†:")
            print("â€¢ æ§åˆ¶æƒé‡çŸ©é˜µçš„æœ€å¤§å¥‡å¼‚å€¼")
            print("â€¢ ç¡®ä¿Lipschitzçº¦æŸ")
            print("â€¢ æé«˜GANè®­ç»ƒç¨³å®šæ€§")
            
            # ç®€åŒ–çš„è°±å½’ä¸€åŒ–å®ç°
            u = [random.gauss(0, 1) for _ in range(len(weight_matrix))]
            v = [random.gauss(0, 1) for _ in range(len(weight_matrix[0]))]
            
            for _ in range(n_iterations):
                # v = W^T u / ||W^T u||
                wt_u = [sum(weight_matrix[i][j] * u[i] for i in range(len(weight_matrix))) 
                       for j in range(len(weight_matrix[0]))]
                norm_wt_u = math.sqrt(sum(x**2 for x in wt_u))
                if norm_wt_u > 0:
                    v = [x / norm_wt_u for x in wt_u]
                
                # u = W v / ||W v||
                w_v = [sum(weight_matrix[i][j] * v[j] for j in range(len(weight_matrix[0]))) 
                      for i in range(len(weight_matrix))]
                norm_w_v = math.sqrt(sum(x**2 for x in w_v))
                if norm_w_v > 0:
                    u = [x / norm_w_v for x in w_v]
            
            # è®¡ç®—è°±èŒƒæ•° Ïƒ = u^T W v
            spectral_norm = sum(u[i] * sum(weight_matrix[i][j] * v[j] 
                                         for j in range(len(weight_matrix[0]))) 
                             for i in range(len(weight_matrix)))
            
            return spectral_norm, u, v
            
        def dropout_bayesian_interpretation(self, dropout_rate=0.5):
            """Dropoutçš„è´å¶æ–¯è§£é‡Š"""
            print(f"\nDropoutçš„è´å¶æ–¯è§£é‡Š:")
            print(f"â€¢ Dropoutç‡: {dropout_rate}")
            print(f"â€¢ ç­‰ä»·äºå¯¹æƒé‡æ–½åŠ å…ˆéªŒåˆ†å¸ƒ")
            print(f"â€¢ è¿‘ä¼¼è´å¶æ–¯æ¨æ–­")
            
            # æ¨¡æ‹Ÿè´å¶æ–¯æƒé‡åˆ†å¸ƒ
            prior_precision = 1.0 / (1 - dropout_rate)
            posterior_variance = 1.0 / prior_precision
            
            print(f"â€¢ å…ˆéªŒç²¾åº¦: {prior_precision:.3f}")
            print(f"â€¢ åéªŒæ–¹å·®: {posterior_variance:.3f}")
            
            return prior_precision, posterior_variance
            
        def weight_decay_l2_regularization(self, weights, lambda_reg=0.01):
            """æƒé‡è¡°å‡ä¸L2æ­£åˆ™åŒ–"""
            print(f"\nL2æ­£åˆ™åŒ–åˆ†æ:")
            print(f"â€¢ æ­£åˆ™åŒ–å‚æ•°Î» = {lambda_reg}")
            
            # è®¡ç®—L2èŒƒæ•°
            l2_norm = sum(w**2 for w in weights)
            l2_penalty = 0.5 * lambda_reg * l2_norm
            
            # L2æ­£åˆ™åŒ–æ¢¯åº¦
            l2_gradient = [lambda_reg * w for w in weights]
            
            print(f"â€¢ L2èŒƒæ•°: {l2_norm:.4f}")
            print(f"â€¢ L2æƒ©ç½šé¡¹: {l2_penalty:.6f}")
            print(f"â€¢ æ¢¯åº¦ä¿®æ­£: {[f'{g:.4f}' for g in l2_gradient[:3]]}...")
            
            return l2_penalty, l2_gradient
    
    # æ­£åˆ™åŒ–æŠ€æœ¯æ¼”ç¤º
    print("é«˜çº§æ­£åˆ™åŒ–æŠ€æœ¯æ¼”ç¤º:")
    
    reg = AdvancedRegularization()
    
    # 1. è°±å½’ä¸€åŒ–æ¼”ç¤º
    weight_matrix = [[0.8, 0.3, 0.2], [0.1, 0.9, 0.4], [0.5, 0.2, 0.7]]
    spectral_norm, u, v = reg.spectral_normalization(weight_matrix)
    
    print(f"æƒé‡çŸ©é˜µ: {weight_matrix}")
    print(f"è°±èŒƒæ•°: {spectral_norm:.4f}")
    
    # 2. Dropoutçš„è´å¶æ–¯è§£é‡Š
    prior_prec, post_var = reg.dropout_bayesian_interpretation(0.3)
    
    # 3. L2æ­£åˆ™åŒ–
    weights = [0.5, -0.8, 0.3, 1.2, -0.4]
    l2_penalty, l2_grad = reg.weight_decay_l2_regularization(weights, 0.01)

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ§® æ·±åº¦å­¦ä¹ æ•°å­¦åŸç†æ·±åº¦è§£æ")
    print("=" * 70)
    
    mathematical_foundations_intro()
    chain_rule_detailed_analysis()
    information_theory_in_deep_learning()
    optimization_theory_advanced()
    variational_inference_theory()
    manifold_learning_theory()
    advanced_regularization_theory()
    
    print("\n" + "=" * 70)
    print("ğŸ¯ æ•°å­¦ç†è®ºæ€»ç»“")
    print()
    print("é€šè¿‡æœ¬æ¨¡å—ä½ å­¦åˆ°äº†:")
    print("â€¢ é“¾å¼æ³•åˆ™åœ¨åå‘ä¼ æ’­ä¸­çš„ç²¾ç¡®åº”ç”¨")
    print("â€¢ ä¿¡æ¯è®ºå¦‚ä½•æŒ‡å¯¼æŸå¤±å‡½æ•°è®¾è®¡")
    print("â€¢ é«˜çº§ä¼˜åŒ–ç®—æ³•çš„æ•°å­¦åŸç†")
    print("â€¢ å˜åˆ†æ¨æ–­åœ¨ç”Ÿæˆæ¨¡å‹ä¸­çš„ä½œç”¨")
    print("â€¢ æµå½¢å­¦ä¹ çš„å‡ ä½•ç›´è§‰")
    print("â€¢ æ­£åˆ™åŒ–æŠ€æœ¯çš„ç†è®ºåŸºç¡€")
    print()
    print("è¿™äº›æ•°å­¦åŸºç¡€æ˜¯ç†è§£å’Œå‘å±•æ–°çš„æ·±åº¦å­¦ä¹ ")
    print("ç®—æ³•çš„å…³é”®ï¼ç»§ç»­æ·±å…¥ç ”ç©¶è¿™äº›ç†è®ºå°†")
    print("å¸®åŠ©ä½ æˆä¸ºçœŸæ­£çš„æ·±åº¦å­¦ä¹ ä¸“å®¶ã€‚")

if __name__ == "__main__":
    main()