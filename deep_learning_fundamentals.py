# -*- coding: utf-8 -*-
"""
æ·±åº¦å­¦ä¹ åŸºç¡€æ•™ç¨‹ - ä»ç¥ç»ç½‘ç»œåˆ°æ·±åº¦å­¦ä¹ 

åŒ…å«ï¼šæ„ŸçŸ¥æœºã€å¤šå±‚æ„ŸçŸ¥æœºã€æ¿€æ´»å‡½æ•°ã€æŸå¤±å‡½æ•°ã€ä¼˜åŒ–ç®—æ³•ç­‰åŸºç¡€æ¦‚å¿µã€‚

æ³¨æ„: æ­¤æ–‡ä»¶å·²ä½œä¸ºå…¼å®¹å…¥å£ï¼Œæ¨èä½¿ç”¨ `from deep_learning.fundamentals import MLP, DeepNetwork`
"""

import random
import math
import json
import warnings

# å¯¼å…¥ utils å·¥å…·å‡½æ•°
from deep_learning.utils import (
    relu, relu_derivative,
    leaky_relu,
    he_normal
)

# æ–°åŒ…è¿ç§»å¼•ç”¨
from deep_learning.fundamentals.deep_network import DeepNetwork
from deep_learning.fundamentals.perceptron import Perceptron

# å…¼å®¹æç¤º
warnings.warn(
    "deep_learning_fundamentals.py å°†è¿ç§»åˆ° deep_learning/fundamentals/ åŒ…ï¼Œ"
    "è¯·ä½¿ç”¨ from deep_learning.fundamentals import MLP, DeepNetwork",
    DeprecationWarning,
    stacklevel=2,
)

def deep_learning_introduction():
    """
    æ·±åº¦å­¦ä¹ å…¥é—¨æ¦‚å¿µ
    
    æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œä½¿ç”¨å¤šå±‚ç¥ç»ç½‘ç»œæ¥å­¦ä¹ æ•°æ®çš„æŠ½è±¡è¡¨ç¤ºã€‚
    
    æ ¸å¿ƒæ¦‚å¿µï¼š
    - æ·±åº¦ï¼šç½‘ç»œæœ‰å¤šä¸ªéšè—å±‚ï¼ˆé€šå¸¸3å±‚ä»¥ä¸Šç§°ä¸ºæ·±åº¦ç½‘ç»œï¼‰
    - è¡¨ç¤ºå­¦ä¹ ï¼šè‡ªåŠ¨å­¦ä¹ æœ‰ç”¨çš„ç‰¹å¾è¡¨ç¤º
    - å±‚æ¬¡åŒ–ç‰¹å¾ï¼šä½å±‚å­¦ä¹ ç®€å•ç‰¹å¾ï¼Œé«˜å±‚å­¦ä¹ å¤æ‚ç‰¹å¾
    - ç«¯åˆ°ç«¯å­¦ä¹ ï¼šä»åŸå§‹æ•°æ®ç›´æ¥å­¦ä¹ åˆ°æœ€ç»ˆè¾“å‡º
    
    æ·±åº¦å­¦ä¹  vs ä¼ ç»Ÿæœºå™¨å­¦ä¹ ï¼š
    1. ç‰¹å¾å·¥ç¨‹ï¼šä¼ ç»ŸMLéœ€è¦æ‰‹å·¥è®¾è®¡ç‰¹å¾ï¼ŒDLè‡ªåŠ¨å­¦ä¹ ç‰¹å¾
    2. æ•°æ®éœ€æ±‚ï¼šDLé€šå¸¸éœ€è¦æ›´å¤šæ•°æ®
    3. è®¡ç®—éœ€æ±‚ï¼šDLéœ€è¦æ›´å¤šè®¡ç®—èµ„æº
    4. æ€§èƒ½ï¼šåœ¨å¤§æ•°æ®é›†ä¸Šï¼ŒDLé€šå¸¸æ€§èƒ½æ›´å¥½
    
    æ·±åº¦å­¦ä¹ å‘å±•å†ç¨‹ï¼š
    - 1950s: æ„ŸçŸ¥æœºçš„è¯ç”Ÿ
    - 1980s: åå‘ä¼ æ’­ç®—æ³•
    - 2006: æ·±åº¦ä¿¡å¿µç½‘ç»œï¼Œæ·±åº¦å­¦ä¹ å¤å…´
    - 2012: AlexNetåœ¨ImageNetè·å¾—çªç ´
    - 2010sè‡³ä»Š: CNNã€RNNã€Transformerç­‰æ¶æ„å‘å±•
    """
    print("=== æ·±åº¦å­¦ä¹ åŸºç¡€æ¦‚å¿µ ===")
    print("æ·±åº¦å­¦ä¹  = å¤šå±‚ç¥ç»ç½‘ç»œ + å¤§æ•°æ® + å¼ºè®¡ç®—åŠ›")
    print()
    
    print("æ·±åº¦å­¦ä¹ çš„ä¸‰å¤§è¦ç´ ï¼š")
    print("1. ç®—æ³•ï¼šç¥ç»ç½‘ç»œæ¶æ„å’Œè®­ç»ƒæ–¹æ³•")
    print("2. æ•°æ®ï¼šå¤§è§„æ¨¡æ ‡æ³¨æ•°æ®é›†")
    print("3. ç®—åŠ›ï¼šGPUå¹¶è¡Œè®¡ç®—èƒ½åŠ›")
    print()
    
    print("ä¸»è¦åº”ç”¨é¢†åŸŸï¼š")
    print("â€¢ è®¡ç®—æœºè§†è§‰ï¼šå›¾åƒåˆ†ç±»ã€ç›®æ ‡æ£€æµ‹ã€å›¾åƒç”Ÿæˆ")
    print("â€¢ è‡ªç„¶è¯­è¨€å¤„ç†ï¼šæœºå™¨ç¿»è¯‘ã€æ–‡æœ¬ç”Ÿæˆã€æƒ…æ„Ÿåˆ†æ")
    print("â€¢ è¯­éŸ³å¤„ç†ï¼šè¯­éŸ³è¯†åˆ«ã€è¯­éŸ³åˆæˆ")
    print("â€¢ æ¨èç³»ç»Ÿï¼šä¸ªæ€§åŒ–æ¨èã€å¹¿å‘ŠæŠ•æ”¾")
    print("â€¢ æ¸¸æˆAIï¼šå›´æ£‹ã€ç”µå­æ¸¸æˆAI")
    print()

def deep_learning_architectures_overview():
    """æ·±åº¦å­¦ä¹ ä¸»è¦æ¶æ„æ¦‚è§ˆ"""
    print("=== æ·±åº¦å­¦ä¹ ä¸»è¦æ¶æ„ ===")
    
    architectures = {
        "å¤šå±‚æ„ŸçŸ¥æœº (MLP)": {
            "description": "æœ€åŸºç¡€çš„æ·±åº¦ç¥ç»ç½‘ç»œ",
            "structure": "å…¨è¿æ¥å±‚å †å ",
            "applications": ["è¡¨æ ¼æ•°æ®åˆ†ç±»", "ç®€å•å›å½’é—®é¢˜"],
            "advantages": ["ç»“æ„ç®€å•", "æ˜“äºç†è§£"],
            "disadvantages": ["å‚æ•°è¿‡å¤š", "éš¾ä»¥å¤„ç†ç©ºé—´ç»“æ„"]
        },
        
        "å·ç§¯ç¥ç»ç½‘ç»œ (CNN)": {
            "description": "ä¸“é—¨å¤„ç†ç½‘æ ¼çŠ¶æ•°æ®ï¼ˆå¦‚å›¾åƒï¼‰",
            "structure": "å·ç§¯å±‚ + æ± åŒ–å±‚ + å…¨è¿æ¥å±‚",
            "applications": ["å›¾åƒåˆ†ç±»", "ç›®æ ‡æ£€æµ‹", "å›¾åƒåˆ†å‰²"],
            "advantages": ["å±€éƒ¨è¿æ¥", "æƒé‡å…±äº«", "å¹³ç§»ä¸å˜æ€§"],
            "disadvantages": ["ä¸»è¦ç”¨äºå›¾åƒ", "éœ€è¦å¤§é‡æ•°æ®"]
        },
        
        "å¾ªç¯ç¥ç»ç½‘ç»œ (RNN)": {
            "description": "å¤„ç†åºåˆ—æ•°æ®çš„ç½‘ç»œæ¶æ„",
            "structure": "å¾ªç¯è¿æ¥çš„éšè—çŠ¶æ€",
            "applications": ["è‡ªç„¶è¯­è¨€å¤„ç†", "æ—¶é—´åºåˆ—é¢„æµ‹", "è¯­éŸ³è¯†åˆ«"],
            "advantages": ["å¤„ç†å˜é•¿åºåˆ—", "è®°å¿†å†å²ä¿¡æ¯"],
            "disadvantages": ["æ¢¯åº¦æ¶ˆå¤±", "éš¾ä»¥å¹¶è¡ŒåŒ–"]
        },
        
        "é•¿çŸ­æœŸè®°å¿†ç½‘ç»œ (LSTM)": {
            "description": "è§£å†³RNNæ¢¯åº¦æ¶ˆå¤±é—®é¢˜çš„æ”¹è¿›ç‰ˆæœ¬",
            "structure": "é—¨æ§æœºåˆ¶ + ç»†èƒçŠ¶æ€",
            "applications": ["æœºå™¨ç¿»è¯‘", "æ–‡æœ¬ç”Ÿæˆ", "æƒ…æ„Ÿåˆ†æ"],
            "advantages": ["é•¿æœŸè®°å¿†èƒ½åŠ›", "ç¼“è§£æ¢¯åº¦æ¶ˆå¤±"],
            "disadvantages": ["ç»“æ„å¤æ‚", "è®¡ç®—å¼€é”€å¤§"]
        },
        
        "Transformer": {
            "description": "åŸºäºæ³¨æ„åŠ›æœºåˆ¶çš„æ¶æ„",
            "structure": "è‡ªæ³¨æ„åŠ› + å‰é¦ˆç½‘ç»œ",
            "applications": ["æœºå™¨ç¿»è¯‘", "æ–‡æœ¬ç”Ÿæˆ", "é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹"],
            "advantages": ["å¹¶è¡ŒåŒ–", "é•¿è·ç¦»ä¾èµ–", "å¯è§£é‡Šæ€§"],
            "disadvantages": ["å†…å­˜æ¶ˆè€—å¤§", "éœ€è¦å¤§é‡æ•°æ®"]
        }
    }
    
    for name, info in architectures.items():
        print(f"\nã€{name}ã€‘")
        print(f"æè¿°: {info['description']}")
        print(f"ç»“æ„: {info['structure']}")
        print(f"åº”ç”¨: {', '.join(info['applications'])}")
        print(f"ä¼˜ç‚¹: {', '.join(info['advantages'])}")
        print(f"ç¼ºç‚¹: {', '.join(info['disadvantages'])}")

def deep_network_challenges():
    """æ·±åº¦ç½‘ç»œè®­ç»ƒæŒ‘æˆ˜"""
    print("\n=== æ·±åº¦ç½‘ç»œè®­ç»ƒæŒ‘æˆ˜ ===")
    
    challenges = {
        "æ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸": {
            "problem": "åå‘ä¼ æ’­æ—¶æ¢¯åº¦è¿‡å°æˆ–è¿‡å¤§",
            "causes": ["æ¿€æ´»å‡½æ•°é¥±å’Œ", "æƒé‡åˆå§‹åŒ–ä¸å½“", "ç½‘ç»œè¿‡æ·±"],
            "solutions": ["ä½¿ç”¨ReLUæ¿€æ´»å‡½æ•°", "æ‰¹å½’ä¸€åŒ–", "æ®‹å·®è¿æ¥", "æ¢¯åº¦è£å‰ª"]
        },
        
        "è¿‡æ‹Ÿåˆ": {
            "problem": "æ¨¡å‹åœ¨è®­ç»ƒé›†ä¸Šè¡¨ç°å¥½ï¼Œåœ¨æµ‹è¯•é›†ä¸Šè¡¨ç°å·®",
            "causes": ["æ¨¡å‹å¤æ‚åº¦è¿‡é«˜", "è®­ç»ƒæ•°æ®ä¸è¶³", "è®­ç»ƒæ—¶é—´è¿‡é•¿"],
            "solutions": ["Dropout", "L1/L2æ­£åˆ™åŒ–", "æ•°æ®å¢å¼º", "æ—©åœæ³•"]
        },
        
        "å†…éƒ¨åå˜é‡åç§»": {
            "problem": "è®­ç»ƒè¿‡ç¨‹ä¸­æ¯å±‚è¾“å…¥åˆ†å¸ƒå‘ç”Ÿå˜åŒ–",
            "causes": ["å‚æ•°æ›´æ–°å¯¼è‡´è¾“å…¥åˆ†å¸ƒå˜åŒ–"],
            "solutions": ["æ‰¹å½’ä¸€åŒ–", "å±‚å½’ä¸€åŒ–", "ç»„å½’ä¸€åŒ–"]
        },
        
        "è®­ç»ƒé€Ÿåº¦æ…¢": {
            "problem": "æ·±åº¦ç½‘ç»œå‚æ•°å¤šï¼Œè®­ç»ƒæ—¶é—´é•¿",
            "causes": ["ç½‘ç»œå¤æ‚åº¦é«˜", "æ•°æ®é‡å¤§", "è®¡ç®—èµ„æºé™åˆ¶"],
            "solutions": ["GPUå¹¶è¡Œ", "åˆ†å¸ƒå¼è®­ç»ƒ", "æ··åˆç²¾åº¦è®­ç»ƒ", "æ¨¡å‹å‹ç¼©"]
        }
    }
    
    for challenge, info in challenges.items():
        print(f"\nã€{challenge}ã€‘")
        print(f"é—®é¢˜: {info['problem']}")
        print(f"åŸå› : {', '.join(info['causes'])}")
        print(f"è§£å†³æ–¹æ¡ˆ: {', '.join(info['solutions'])}")

def modern_training_techniques():
    """ç°ä»£è®­ç»ƒæŠ€å·§"""
    print("\n=== ç°ä»£æ·±åº¦å­¦ä¹ è®­ç»ƒæŠ€å·§ ===")
    
    techniques = {
        "ä¼˜åŒ–ç®—æ³•": [
            "SGD + åŠ¨é‡: åŠ é€Ÿæ”¶æ•›ï¼Œå‡å°‘éœ‡è¡",
            "Adam: è‡ªé€‚åº”å­¦ä¹ ç‡ï¼Œé€‚ç”¨äºå¤§å¤šæ•°é—®é¢˜",
            "AdamW: Adam + æƒé‡è¡°å‡ï¼Œæ›´å¥½çš„æ­£åˆ™åŒ–",
            "å­¦ä¹ ç‡è°ƒåº¦: ä½™å¼¦é€€ç«ã€åˆ†æ®µè¡°å‡"
        ],
        
        "æ­£åˆ™åŒ–æŠ€æœ¯": [
            "Dropout: éšæœºä¸¢å¼ƒç¥ç»å…ƒï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ",
            "Batch Normalization: å½’ä¸€åŒ–è¾“å…¥ï¼ŒåŠ é€Ÿè®­ç»ƒ",
            "Data Augmentation: æ•°æ®å¢å¼ºï¼Œå¢åŠ æ•°æ®å¤šæ ·æ€§",
            "Early Stopping: åœ¨éªŒè¯é›†æ€§èƒ½ä¸‹é™æ—¶åœæ­¢è®­ç»ƒ"
        ],
        
        "ç½‘ç»œè®¾è®¡": [
            "æ®‹å·®è¿æ¥ (ResNet): è§£å†³æ·±åº¦ç½‘ç»œé€€åŒ–é—®é¢˜",
            "å¯†é›†è¿æ¥ (DenseNet): ç‰¹å¾é‡ç”¨ï¼Œå‚æ•°æ•ˆç‡",
            "æ³¨æ„åŠ›æœºåˆ¶: åŠ¨æ€å…³æ³¨é‡è¦ä¿¡æ¯",
            "è·³è·ƒè¿æ¥: è¿æ¥ä¸åŒå±‚ï¼Œä¿¡æ¯æµåŠ¨æ›´å¥½"
        ],
        
        "è®­ç»ƒç­–ç•¥": [
            "é¢„è®­ç»ƒ + å¾®è°ƒ: åˆ©ç”¨é¢„è®­ç»ƒæ¨¡å‹ï¼Œå‡å°‘è®­ç»ƒæ—¶é—´",
            "è¿ç§»å­¦ä¹ : å°†çŸ¥è¯†ä»ä¸€ä¸ªä»»åŠ¡è¿ç§»åˆ°å¦ä¸€ä¸ªä»»åŠ¡",
            "å¤šä»»åŠ¡å­¦ä¹ : åŒæ—¶å­¦ä¹ å¤šä¸ªç›¸å…³ä»»åŠ¡",
            "è‡ªç›‘ç£å­¦ä¹ : ä»æ— æ ‡ç­¾æ•°æ®ä¸­å­¦ä¹ è¡¨ç¤º"
        ]
    }
    
    for category, items in techniques.items():
        print(f"\nã€{category}ã€‘")
        for item in items:
            print(f"â€¢ {item}")

class DeepNetwork:
    """
    æ·±åº¦ç¥ç»ç½‘ç»œå®ç°
    æ”¯æŒå¤šç§ç°ä»£æŠ€æœ¯
    """
    
    def __init__(self, layers, learning_rate=0.001, activation='relu', 
                 use_batch_norm=False, dropout_rate=0.0):
        """
        åˆå§‹åŒ–æ·±åº¦ç½‘ç»œ
        
        å‚æ•°:
        layers: æ¯å±‚ç¥ç»å…ƒæ•°é‡
        learning_rate: å­¦ä¹ ç‡
        activation: æ¿€æ´»å‡½æ•°
        use_batch_norm: æ˜¯å¦ä½¿ç”¨æ‰¹å½’ä¸€åŒ–
        dropout_rate: Dropoutæ¯”ç‡
        """
        self.layers = layers
        self.learning_rate = learning_rate
        self.activation = activation
        self.use_batch_norm = use_batch_norm
        self.dropout_rate = dropout_rate
        
        # ä½¿ç”¨Heåˆå§‹åŒ–ï¼ˆé€‚åˆReLUï¼‰
        self.weights = []
        self.biases = []

        for i in range(len(layers) - 1):
            # ä½¿ç”¨ utils.he_normal è¿›è¡Œæƒé‡åˆå§‹åŒ–
            weights = he_normal((layers[i], layers[i + 1]))
            self.weights.append(weights)

            # åç½®åˆå§‹åŒ–ä¸º0
            biases = [0.0 for _ in range(layers[i + 1])]
            self.biases.append(biases)
        
        # æ‰¹å½’ä¸€åŒ–å‚æ•°
        if use_batch_norm:
            self.bn_gamma = []  # ç¼©æ”¾å‚æ•°
            self.bn_beta = []   # åç§»å‚æ•°
            self.bn_running_mean = []  # è¿è¡Œæ—¶å‡å€¼
            self.bn_running_var = []   # è¿è¡Œæ—¶æ–¹å·®
            
            for i in range(len(layers) - 1):
                self.bn_gamma.append([1.0 for _ in range(layers[i + 1])])
                self.bn_beta.append([0.0 for _ in range(layers[i + 1])])
                self.bn_running_mean.append([0.0 for _ in range(layers[i + 1])])
                self.bn_running_var.append([1.0 for _ in range(layers[i + 1])])
        
        # è®­ç»ƒå†å²
        self.loss_history = []
        self.val_loss_history = []
        
        print(f"æ·±åº¦ç½‘ç»œåˆå§‹åŒ–å®Œæˆ:")
        print(f"ç»“æ„: {' -> '.join(map(str, layers))}")
        print(f"æ¿€æ´»å‡½æ•°: {activation}")
        print(f"æ‰¹å½’ä¸€åŒ–: {'æ˜¯' if use_batch_norm else 'å¦'}")
        print(f"Dropout: {dropout_rate}")
        print(f"æ€»å‚æ•°é‡: {self.count_parameters()}")
    
    def count_parameters(self):
        """è®¡ç®—æ€»å‚æ•°æ•°é‡"""
        total = 0
        for i in range(len(self.weights)):
            total += len(self.weights[i]) * len(self.weights[i][0])
            total += len(self.biases[i])
        
        if self.use_batch_norm:
            for i in range(len(self.bn_gamma)):
                total += len(self.bn_gamma[i]) * 2  # gammaå’Œbeta
        
        return total
    
    # æ³¨æ„: æ¿€æ´»å‡½æ•°ç°åœ¨ä» deep_learning.utils å¯¼å…¥
    # relu, leaky_relu ç­‰å‡½æ•°å·²åœ¨æ¨¡å—é¡¶éƒ¨å¯¼å…¥

    def batch_normalize(self, x, layer_idx, training=True, momentum=0.9, eps=1e-8):
        """æ‰¹å½’ä¸€åŒ–"""
        if not self.use_batch_norm:
            return x
        
        if training:
            # è®¡ç®—æ‰¹ç»Ÿè®¡é‡
            mean = sum(x) / len(x)
            var = sum((xi - mean) ** 2 for xi in x) / len(x)
            
            # æ›´æ–°è¿è¡Œæ—¶ç»Ÿè®¡é‡
            self.bn_running_mean[layer_idx] = [
                momentum * rm + (1 - momentum) * mean 
                for rm in self.bn_running_mean[layer_idx]
            ]
            self.bn_running_var[layer_idx] = [
                momentum * rv + (1 - momentum) * var 
                for rv in self.bn_running_var[layer_idx]
            ]
        else:
            # ä½¿ç”¨è¿è¡Œæ—¶ç»Ÿè®¡é‡
            mean = sum(self.bn_running_mean[layer_idx]) / len(self.bn_running_mean[layer_idx])
            var = sum(self.bn_running_var[layer_idx]) / len(self.bn_running_var[layer_idx])
        
        # å½’ä¸€åŒ–
        x_norm = [(xi - mean) / math.sqrt(var + eps) for xi in x]
        
        # ç¼©æ”¾å’Œåç§»
        output = []
        for i, xi in enumerate(x_norm):
            if i < len(self.bn_gamma[layer_idx]):
                out = self.bn_gamma[layer_idx][i] * xi + self.bn_beta[layer_idx][i]
                output.append(out)
            else:
                output.append(xi)
        
        return output
    
    def dropout(self, x, training=True):
        """Dropoutæ­£åˆ™åŒ–"""
        if not training or self.dropout_rate == 0:
            return x
        
        # éšæœºä¸¢å¼ƒç¥ç»å…ƒ
        mask = [1 if random.random() > self.dropout_rate else 0 for _ in x]
        scale = 1.0 / (1.0 - self.dropout_rate)  # ç¼©æ”¾è¡¥å¿
        
        return [xi * mi * scale for xi, mi in zip(x, mask)]
    
    def forward(self, inputs, training=True):
        """å‰å‘ä¼ æ’­"""
        current = inputs
        activations = [inputs]
        z_values = []
        
        for i in range(len(self.weights)):
            # çº¿æ€§å˜æ¢
            z = []
            for j in range(len(self.weights[i])):
                weighted_sum = sum(w * inp for w, inp in zip(self.weights[i][j], current))
                weighted_sum += self.biases[i][j]
                z.append(weighted_sum)
            
            z_values.append(z)
            
            # æ‰¹å½’ä¸€åŒ–
            if self.use_batch_norm and i < len(self.weights) - 1:  # ä¸åœ¨è¾“å‡ºå±‚ä½¿ç”¨
                z = self.batch_normalize(z, i, training)
            
            # æ¿€æ´»å‡½æ•°
            if i < len(self.weights) - 1:  # éšè—å±‚
                if self.activation == 'relu':
                    activated = [relu(zi) for zi in z]
                elif self.activation == 'leaky_relu':
                    activated = [leaky_relu(zi) for zi in z]
                else:
                    activated = z  # çº¿æ€§

                # Dropout
                activated = self.dropout(activated, training)
            else:  # è¾“å‡ºå±‚
                activated = z  # è¾“å‡ºå±‚é€šå¸¸ä¸ä½¿ç”¨æ¿€æ´»å‡½æ•°
            
            activations.append(activated)
            current = activated
        
        return activations, z_values
    
    def predict(self, inputs):
        """é¢„æµ‹"""
        activations, _ = self.forward(inputs, training=False)
        return activations[-1]
    
    def train_batch(self, X_batch, y_batch):
        """è®­ç»ƒä¸€ä¸ªæ‰¹æ¬¡"""
        total_loss = 0
        
        for i in range(len(X_batch)):
            inputs = X_batch[i]
            targets = y_batch[i] if isinstance(y_batch[i], list) else [y_batch[i]]
            
            # å‰å‘ä¼ æ’­
            activations, z_values = self.forward(inputs, training=True)
            predictions = activations[-1]
            
            # è®¡ç®—æŸå¤±
            loss = sum((pred - target) ** 2 for pred, target in zip(predictions, targets)) / len(targets)
            total_loss += loss
            
            # åå‘ä¼ æ’­ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
            # è¿™é‡Œå¯ä»¥å®ç°å®Œæ•´çš„åå‘ä¼ æ’­ç®—æ³•
            # ä¸ºäº†ç®€åŒ–ï¼Œæˆ‘ä»¬ä½¿ç”¨ç®€å•çš„æ¢¯åº¦è¿‘ä¼¼
            
        return total_loss / len(X_batch)

def transfer_learning_concept():
    """è¿ç§»å­¦ä¹ æ¦‚å¿µ"""
    print("\n=== è¿ç§»å­¦ä¹  ===")
    
    print("è¿ç§»å­¦ä¹ çš„æ ¸å¿ƒæ€æƒ³ï¼š")
    print("åˆ©ç”¨åœ¨å¤§æ•°æ®é›†ä¸Šé¢„è®­ç»ƒçš„æ¨¡å‹ï¼Œè¿ç§»åˆ°æ–°çš„ä»»åŠ¡ä¸Š")
    print()
    
    print("è¿ç§»å­¦ä¹ çš„ä¼˜åŠ¿ï¼š")
    print("â€¢ å‡å°‘è®­ç»ƒæ—¶é—´å’Œè®¡ç®—èµ„æºéœ€æ±‚")
    print("â€¢ åœ¨å°æ•°æ®é›†ä¸Šä¹Ÿèƒ½å–å¾—å¥½æ•ˆæœ")  
    print("â€¢ åˆ©ç”¨é¢„è®­ç»ƒæ¨¡å‹å­¦åˆ°çš„é€šç”¨ç‰¹å¾")
    print("â€¢ æé«˜æ¨¡å‹æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›")
    print()
    
    print("è¿ç§»å­¦ä¹ çš„æ–¹å¼ï¼š")
    print("1. ç‰¹å¾æå–: å†»ç»“é¢„è®­ç»ƒæ¨¡å‹ï¼Œåªè®­ç»ƒåˆ†ç±»å™¨")
    print("2. å¾®è°ƒ: å¯¹é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œç»†å¾®è°ƒæ•´")
    print("3. ç«¯åˆ°ç«¯å¾®è°ƒ: å¯¹æ•´ä¸ªç½‘ç»œè¿›è¡Œå¾®è°ƒ")
    print()
    
    print("é¢„è®­ç»ƒæ¨¡å‹ç¤ºä¾‹ï¼š")
    print("â€¢ è®¡ç®—æœºè§†è§‰: ResNet, VGG, EfficientNet")
    print("â€¢ è‡ªç„¶è¯­è¨€å¤„ç†: BERT, GPT, T5")
    print("â€¢ è¯­éŸ³è¯†åˆ«: Wav2Vec, Whisper")

def deep_learning_frameworks():
    """æ·±åº¦å­¦ä¹ æ¡†æ¶ä»‹ç»"""
    print("\n=== æ·±åº¦å­¦ä¹ æ¡†æ¶ ===")
    
    frameworks = {
        "TensorFlow/Keras": {
            "ç‰¹ç‚¹": ["Googleå¼€å‘", "å·¥ä¸šçº§éƒ¨ç½²", "æ˜“äºä½¿ç”¨çš„é«˜çº§API"],
            "ä¼˜åŠ¿": ["ç”Ÿæ€å®Œå–„", "éƒ¨ç½²ä¾¿åˆ©", "ç¤¾åŒºæ´»è·ƒ"],
            "é€‚ç”¨": ["å·¥ä¸šåº”ç”¨", "åˆå­¦è€…", "ç ”ç©¶"]
        },
        
        "PyTorch": {
            "ç‰¹ç‚¹": ["Facebookå¼€å‘", "åŠ¨æ€è®¡ç®—å›¾", "Pythonicè®¾è®¡"],
            "ä¼˜åŠ¿": ["çµæ´»æ€§é«˜", "è°ƒè¯•æ–¹ä¾¿", "ç ”ç©¶å‹å¥½"],
            "é€‚ç”¨": ["å­¦æœ¯ç ”ç©¶", "å¿«é€ŸåŸå‹", "å¤æ‚æ¨¡å‹"]
        },
        
        "JAX": {
            "ç‰¹ç‚¹": ["Googleå¼€å‘", "å‡½æ•°å¼ç¼–ç¨‹", "XLAç¼–è¯‘"],
            "ä¼˜åŠ¿": ["æ€§èƒ½ä¼˜ç§€", "è‡ªåŠ¨å¾®åˆ†", "æ•°å€¼è®¡ç®—"],
            "é€‚ç”¨": ["é«˜æ€§èƒ½è®¡ç®—", "ç§‘å­¦è®¡ç®—", "ç ”ç©¶"]
        }
    }
    
    for name, info in frameworks.items():
        print(f"\nã€{name}ã€‘")
        print(f"ç‰¹ç‚¹: {', '.join(info['ç‰¹ç‚¹'])}")
        print(f"ä¼˜åŠ¿: {', '.join(info['ä¼˜åŠ¿'])}")
        print(f"é€‚ç”¨: {', '.join(info['é€‚ç”¨'])}")

def learning_roadmap():
    """æ·±åº¦å­¦ä¹ å­¦ä¹ è·¯çº¿å›¾"""
    print("\n=== æ·±åº¦å­¦ä¹ å­¦ä¹ è·¯çº¿å›¾ ===")
    
    roadmap = {
        "åŸºç¡€é˜¶æ®µ (1-2ä¸ªæœˆ)": [
            "ç†è§£ç¥ç»ç½‘ç»œåŸºæœ¬åŸç†",
            "æŒæ¡åå‘ä¼ æ’­ç®—æ³•",
            "ç†Ÿæ‚‰æ¿€æ´»å‡½æ•°å’ŒæŸå¤±å‡½æ•°",
            "å­¦ä¹ åŸºæœ¬çš„æ­£åˆ™åŒ–æŠ€æœ¯",
            "å®ç°ç®€å•çš„å¤šå±‚æ„ŸçŸ¥æœº"
        ],
        
        "è¿›é˜¶é˜¶æ®µ (2-3ä¸ªæœˆ)": [
            "å­¦ä¹ å·ç§¯ç¥ç»ç½‘ç»œ(CNN)",
            "ç†è§£å¾ªç¯ç¥ç»ç½‘ç»œ(RNN/LSTM)",
            "æŒæ¡ç°ä»£è®­ç»ƒæŠ€å·§",
            "å­¦ä¹ ä½¿ç”¨æ·±åº¦å­¦ä¹ æ¡†æ¶",
            "å®Œæˆå›¾åƒåˆ†ç±»å’Œæ–‡æœ¬åˆ†ç±»é¡¹ç›®"
        ],
        
        "é«˜çº§é˜¶æ®µ (3-4ä¸ªæœˆ)": [
            "å­¦ä¹ Transformeræ¶æ„",
            "ç†è§£æ³¨æ„åŠ›æœºåˆ¶",
            "æŒæ¡é¢„è®­ç»ƒå’Œå¾®è°ƒ",
            "å­¦ä¹ ç”Ÿæˆå¯¹æŠ—ç½‘ç»œ(GAN)",
            "å®Œæˆå¤æ‚çš„ç«¯åˆ°ç«¯é¡¹ç›®"
        ],
        
        "ä¸“å®¶é˜¶æ®µ (æŒç»­å­¦ä¹ )": [
            "è·Ÿè¸ªæœ€æ–°ç ”ç©¶è¿›å±•",
            "å­¦ä¹ ç‰¹å®šé¢†åŸŸçš„å…ˆè¿›æŠ€æœ¯",
            "å‚ä¸å¼€æºé¡¹ç›®è´¡çŒ®",
            "å‘è¡¨å­¦æœ¯è®ºæ–‡æˆ–æŠ€æœ¯åšå®¢",
            "è§£å†³å®é™…å·¥ä¸šé—®é¢˜"
        ]
    }
    
    for stage, tasks in roadmap.items():
        print(f"\nã€{stage}ã€‘")
        for i, task in enumerate(tasks, 1):
            print(f"{i}. {task}")

def practical_tips():
    """å®ç”¨æŠ€å·§å’Œå»ºè®®"""
    print("\n=== å®ç”¨æŠ€å·§å’Œå»ºè®® ===")
    
    print("ç¼–ç¨‹å®è·µæŠ€å·§ï¼š")
    print("â€¢ ä»ç®€å•æ¨¡å‹å¼€å§‹ï¼Œé€æ­¥å¢åŠ å¤æ‚åº¦")
    print("â€¢ ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹ä½œä¸ºèµ·ç‚¹")
    print("â€¢ é‡è§†æ•°æ®è´¨é‡å’Œé¢„å¤„ç†")
    print("â€¢ å»ºç«‹å®Œå–„çš„å®éªŒè®°å½•ç³»ç»Ÿ")
    print("â€¢ å¤šåšæ¶ˆèå®éªŒåˆ†ææ¨¡å‹æ€§èƒ½")
    print()
    
    print("è°ƒè¯•æŠ€å·§ï¼š")
    print("â€¢ æ£€æŸ¥æ•°æ®åŠ è½½å’Œé¢„å¤„ç†æµç¨‹")
    print("â€¢ éªŒè¯æ¨¡å‹è¾“å‡ºå½¢çŠ¶å’Œæ•°å€¼èŒƒå›´")
    print("â€¢ ç›‘æ§æ¢¯åº¦å¤§å°å’Œåˆ†å¸ƒ")
    print("â€¢ å¯è§†åŒ–ä¸­é—´å±‚ç‰¹å¾å›¾")
    print("â€¢ å¯¹æ¯”ç®€åŒ–ç‰ˆæœ¬çš„æ¨¡å‹æ€§èƒ½")
    print()
    
    print("æ€§èƒ½ä¼˜åŒ–ï¼š")
    print("â€¢ ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ")
    print("â€¢ ä¼˜åŒ–æ•°æ®åŠ è½½æµæ°´çº¿")
    print("â€¢ åˆç†è®¾ç½®æ‰¹å¤§å°")
    print("â€¢ ä½¿ç”¨åˆ†å¸ƒå¼è®­ç»ƒ")
    print("â€¢ è€ƒè™‘æ¨¡å‹å‹ç¼©å’Œé‡åŒ–")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ æ·±åº¦å­¦ä¹ åŸºç¡€æ•™ç¨‹")
    print("=" * 50)
    
    deep_learning_introduction()
    deep_learning_architectures_overview()
    deep_network_challenges()
    modern_training_techniques()
    transfer_learning_concept()
    deep_learning_frameworks()
    learning_roadmap()
    practical_tips()
    
    print("\n" + "=" * 50)
    print("ğŸ¯ æ€»ç»“")
    print("æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„é‡è¦åˆ†æ”¯ï¼š")
    print("â€¢ ä½¿ç”¨å¤šå±‚ç¥ç»ç½‘ç»œå­¦ä¹ å¤æ‚æ¨¡å¼")
    print("â€¢ åœ¨å¤§æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚")
    print("â€¢ éœ€è¦æŒæ¡ç°ä»£è®­ç»ƒæŠ€å·§")
    print("â€¢ å¹¿æ³›åº”ç”¨äºå„ä¸ªé¢†åŸŸ")
    print()
    print("å­¦ä¹ å»ºè®®ï¼š")
    print("â€¢ ç†è®ºä¸å®è·µå¹¶é‡")
    print("â€¢ å¤šåšé¡¹ç›®å·©å›ºçŸ¥è¯†")
    print("â€¢ å…³æ³¨æœ€æ–°ç ”ç©¶è¿›å±•")
    print("â€¢ å‚ä¸å¼€æºç¤¾åŒº")
    print()
    print("ä¸‹ä¸€æ­¥ï¼šé€‰æ‹©å…·ä½“æ–¹å‘æ·±å…¥å­¦ä¹ ï¼")
    print("æ¨èï¼šå…ˆå­¦ä¹ CNNå¤„ç†å›¾åƒï¼Œæˆ–å­¦ä¹ Transformerå¤„ç†æ–‡æœ¬")

if __name__ == "__main__":
    main()
