# -*- coding: utf-8 -*-
"""DeepNetwork å®ç°ï¼ˆè¿ç§»è‡ª deep_learning_fundamentals.pyï¼‰"""

import math
import random

from deep_learning.utils import (
    relu, leaky_relu,
    he_normal,
)

class DeepNetwork:
    """
    æ·±åº¦ç¥ç»ç½‘ç»œå®ç°
    æ”¯æŒå¤šç§ç°ä»£æŠ€æœ¯
    """
    
    def __init__(self, layers, learning_rate=0.001, activation='relu', 
                 use_batch_norm=False, dropout_rate=0.0):
        """
        åˆå§‹åŒ–æ·±åº¦ç½‘ç»œ
        
        å‚æ•°:
        layers: æ¯å±‚ç¥ç»å…ƒæ•°é‡
        learning_rate: å­¦ä¹ ç‡
        activation: æ¿€æ´»å‡½æ•°
        use_batch_norm: æ˜¯å¦ä½¿ç”¨æ‰¹å½’ä¸€åŒ–
        dropout_rate: Dropoutæ¯”ç‡
        """
        self.layers = layers
        self.learning_rate = learning_rate
        self.activation = activation
        self.use_batch_norm = use_batch_norm
        self.dropout_rate = dropout_rate
        
        # ä½¿ç”¨Heåˆå§‹åŒ–ï¼ˆé€‚åˆReLUï¼‰
        self.weights = []
        self.biases = []

        for i in range(len(layers) - 1):
            # ä½¿ç”¨ utils.he_normal è¿›è¡Œæƒé‡åˆå§‹åŒ–
            weights = he_normal((layers[i], layers[i + 1]))
            self.weights.append(weights)

            # åç½®åˆå§‹åŒ–ä¸º0
            biases = [0.0 for _ in range(layers[i + 1])]
            self.biases.append(biases)
        
        # æ‰¹å½’ä¸€åŒ–å‚æ•°
        if use_batch_norm:
            self.bn_gamma = []  # ç¼©æ”¾å‚æ•°
            self.bn_beta = []   # åç§»å‚æ•°
            self.bn_running_mean = []  # è¿è¡Œæ—¶å‡å€¼
            self.bn_running_var = []   # è¿è¡Œæ—¶æ–¹å·®
            
            for i in range(len(layers) - 1):
                self.bn_gamma.append([1.0 for _ in range(layers[i + 1])])
                self.bn_beta.append([0.0 for _ in range(layers[i + 1])])
                self.bn_running_mean.append([0.0 for _ in range(layers[i + 1])])
                self.bn_running_var.append([1.0 for _ in range(layers[i + 1])])
        
        # è®­ç»ƒå†å²
        self.loss_history = []
        self.val_loss_history = []
        
        print(f"æ·±åº¦ç½‘ç»œåˆå§‹åŒ–å®Œæˆ:")
        print(f"ç»“æ„: {' -> '.join(map(str, layers))}")
        print(f"æ¿€æ´»å‡½æ•°: {activation}")
        print(f"æ‰¹å½’ä¸€åŒ–: {'æ˜¯' if use_batch_norm else 'å¦'}")
        print(f"Dropout: {dropout_rate}")
        print(f"æ€»å‚æ•°é‡: {self.count_parameters()}")
    
    def count_parameters(self):
        """è®¡ç®—æ€»å‚æ•°æ•°é‡"""
        total = 0
        for i in range(len(self.weights)):
            total += len(self.weights[i]) * len(self.weights[i][0])
            total += len(self.biases[i])
        
        if self.use_batch_norm:
            for i in range(len(self.bn_gamma)):
                total += len(self.bn_gamma[i]) * 2  # gammaå’Œbeta
        
        return total
    
    # æ³¨æ„: æ¿€æ´»å‡½æ•°ç°åœ¨ä» deep_learning.utils å¯¼å…¥
    # relu, leaky_relu ç­‰å‡½æ•°å·²åœ¨æ¨¡å—é¡¶éƒ¨å¯¼å…¥

    def batch_normalize(self, x, layer_idx, training=True, momentum=0.9, eps=1e-8):
        """æ‰¹å½’ä¸€åŒ–"""
        if not self.use_batch_norm:
            return x
        
        if training:
            # è®¡ç®—æ‰¹ç»Ÿè®¡é‡
            mean = sum(x) / len(x)
            var = sum((xi - mean) ** 2 for xi in x) / len(x)
            
            # æ›´æ–°è¿è¡Œæ—¶ç»Ÿè®¡é‡
            self.bn_running_mean[layer_idx] = [
                momentum * rm + (1 - momentum) * mean 
                for rm in self.bn_running_mean[layer_idx]
            ]
            self.bn_running_var[layer_idx] = [
                momentum * rv + (1 - momentum) * var 
                for rv in self.bn_running_var[layer_idx]
            ]
        else:
            # ä½¿ç”¨è¿è¡Œæ—¶ç»Ÿè®¡é‡
            mean = sum(self.bn_running_mean[layer_idx]) / len(self.bn_running_mean[layer_idx])
            var = sum(self.bn_running_var[layer_idx]) / len(self.bn_running_var[layer_idx])
        
        # å½’ä¸€åŒ–
        x_norm = [(xi - mean) / math.sqrt(var + eps) for xi in x]
        
        # ç¼©æ”¾å’Œåç§»
        output = []
        for i, xi in enumerate(x_norm):
            if i < len(self.bn_gamma[layer_idx]):
                out = self.bn_gamma[layer_idx][i] * xi + self.bn_beta[layer_idx][i]
                output.append(out)
            else:
                output.append(xi)
        
        return output
    
    def dropout(self, x, training=True):
        """Dropoutæ­£åˆ™åŒ–"""
        if not training or self.dropout_rate == 0:
            return x
        
        # éšæœºä¸¢å¼ƒç¥ç»å…ƒ
        mask = [1 if random.random() > self.dropout_rate else 0 for _ in x]
        scale = 1.0 / (1.0 - self.dropout_rate)  # ç¼©æ”¾è¡¥å¿
        
        return [xi * mi * scale for xi, mi in zip(x, mask)]
    
    def forward(self, inputs, training=True):
        """å‰å‘ä¼ æ’­"""
        current = inputs
        activations = [inputs]
        z_values = []
        
        for i in range(len(self.weights)):
            # çº¿æ€§å˜æ¢ (æƒé‡çŸ©é˜µå½¢çŠ¶: è¾“å…¥ç»´åº¦ x è¾“å‡ºç»´åº¦)
            out_dim = len(self.weights[i][0])
            in_dim = len(self.weights[i])
            z = []
            for j in range(out_dim):
                weighted_sum = 0.0
                for k in range(in_dim):
                    weighted_sum += self.weights[i][k][j] * current[k]
                weighted_sum += self.biases[i][j]
                z.append(weighted_sum)
            
            z_values.append(z)
            
            # æ‰¹å½’ä¸€åŒ–
            if self.use_batch_norm and i < len(self.weights) - 1:  # ä¸åœ¨è¾“å‡ºå±‚ä½¿ç”¨
                z = self.batch_normalize(z, i, training)
            
            # æ¿€æ´»å‡½æ•°
            if i < len(self.weights) - 1:  # éšè—å±‚
                if self.activation == 'relu':
                    activated = [relu(zi) for zi in z]
                elif self.activation == 'leaky_relu':
                    activated = [leaky_relu(zi) for zi in z]
                else:
                    activated = z  # çº¿æ€§

                # Dropout
                activated = self.dropout(activated, training)
            else:  # è¾“å‡ºå±‚
                activated = z  # è¾“å‡ºå±‚é€šå¸¸ä¸ä½¿ç”¨æ¿€æ´»å‡½æ•°
            
            activations.append(activated)
            current = activated
        
        return activations, z_values

    def predict(self, inputs):
        """é¢„æµ‹"""
        activations, _ = self.forward(inputs, training=False)
        return activations[-1]

    def forward_batch(self, batch_inputs, training=False):
        """æ‰¹é‡å‰å‘ï¼Œè¿”å›æ¯ä¸ªæ ·æœ¬çš„è¾“å‡º"""
        outputs = []
        for sample in batch_inputs:
            outputs.append(self.predict(sample) if not training else self.forward(sample, training=training)[0][-1])
        return outputs

    def train_batch(self, X_batch, y_batch):
        """è®­ç»ƒä¸€ä¸ªæ‰¹æ¬¡"""
        total_loss = 0
        
        for i in range(len(X_batch)):
            inputs = X_batch[i]
            targets = y_batch[i] if isinstance(y_batch[i], list) else [y_batch[i]]
            
            # å‰å‘ä¼ æ’­
            activations, z_values = self.forward(inputs, training=True)
            predictions = activations[-1]
            
            # è®¡ç®—æŸå¤±
            loss = sum((pred - target) ** 2 for pred, target in zip(predictions, targets)) / len(targets)
            total_loss += loss
            
            # åå‘ä¼ æ’­ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
            # è¿™é‡Œå¯ä»¥å®ç°å®Œæ•´çš„åå‘ä¼ æ’­ç®—æ³•
            # ä¸ºäº†ç®€åŒ–ï¼Œæˆ‘ä»¬ä½¿ç”¨ç®€å•çš„æ¢¯åº¦è¿‘ä¼¼
            
        return total_loss / len(X_batch)

def transfer_learning_concept():
    """è¿ç§»å­¦ä¹ æ¦‚å¿µ"""
    print("\n=== è¿ç§»å­¦ä¹  ===")
    
    print("è¿ç§»å­¦ä¹ çš„æ ¸å¿ƒæ€æƒ³ï¼š")
    print("åˆ©ç”¨åœ¨å¤§æ•°æ®é›†ä¸Šé¢„è®­ç»ƒçš„æ¨¡å‹ï¼Œè¿ç§»åˆ°æ–°çš„ä»»åŠ¡ä¸Š")
    print()
    
    print("è¿ç§»å­¦ä¹ çš„ä¼˜åŠ¿ï¼š")
    print("â€¢ å‡å°‘è®­ç»ƒæ—¶é—´å’Œè®¡ç®—èµ„æºéœ€æ±‚")
    print("â€¢ åœ¨å°æ•°æ®é›†ä¸Šä¹Ÿèƒ½å–å¾—å¥½æ•ˆæœ")  
    print("â€¢ åˆ©ç”¨é¢„è®­ç»ƒæ¨¡å‹å­¦åˆ°çš„é€šç”¨ç‰¹å¾")
    print("â€¢ æé«˜æ¨¡å‹æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›")
    print()
    
    print("è¿ç§»å­¦ä¹ çš„æ–¹å¼ï¼š")
    print("1. ç‰¹å¾æå–: å†»ç»“é¢„è®­ç»ƒæ¨¡å‹ï¼Œåªè®­ç»ƒåˆ†ç±»å™¨")
    print("2. å¾®è°ƒ: å¯¹é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œç»†å¾®è°ƒæ•´")
    print("3. ç«¯åˆ°ç«¯å¾®è°ƒ: å¯¹æ•´ä¸ªç½‘ç»œè¿›è¡Œå¾®è°ƒ")
    print()
    
    print("é¢„è®­ç»ƒæ¨¡å‹ç¤ºä¾‹ï¼š")
    print("â€¢ è®¡ç®—æœºè§†è§‰: ResNet, VGG, EfficientNet")
    print("â€¢ è‡ªç„¶è¯­è¨€å¤„ç†: BERT, GPT, T5")
    print("â€¢ è¯­éŸ³è¯†åˆ«: Wav2Vec, Whisper")

def deep_learning_frameworks():
    """æ·±åº¦å­¦ä¹ æ¡†æ¶ä»‹ç»"""
    print("\n=== æ·±åº¦å­¦ä¹ æ¡†æ¶ ===")
    
    frameworks = {
        "TensorFlow/Keras": {
            "ç‰¹ç‚¹": ["Googleå¼€å‘", "å·¥ä¸šçº§éƒ¨ç½²", "æ˜“äºä½¿ç”¨çš„é«˜çº§API"],
            "ä¼˜åŠ¿": ["ç”Ÿæ€å®Œå–„", "éƒ¨ç½²ä¾¿åˆ©", "ç¤¾åŒºæ´»è·ƒ"],
            "é€‚ç”¨": ["å·¥ä¸šåº”ç”¨", "åˆå­¦è€…", "ç ”ç©¶"]
        },
        
        "PyTorch": {
            "ç‰¹ç‚¹": ["Facebookå¼€å‘", "åŠ¨æ€è®¡ç®—å›¾", "Pythonicè®¾è®¡"],
            "ä¼˜åŠ¿": ["çµæ´»æ€§é«˜", "è°ƒè¯•æ–¹ä¾¿", "ç ”ç©¶å‹å¥½"],
            "é€‚ç”¨": ["å­¦æœ¯ç ”ç©¶", "å¿«é€ŸåŸå‹", "å¤æ‚æ¨¡å‹"]
        },
        
        "JAX": {
            "ç‰¹ç‚¹": ["Googleå¼€å‘", "å‡½æ•°å¼ç¼–ç¨‹", "XLAç¼–è¯‘"],
            "ä¼˜åŠ¿": ["æ€§èƒ½ä¼˜ç§€", "è‡ªåŠ¨å¾®åˆ†", "æ•°å€¼è®¡ç®—"],
            "é€‚ç”¨": ["é«˜æ€§èƒ½è®¡ç®—", "ç§‘å­¦è®¡ç®—", "ç ”ç©¶"]
        }
    }
    
    for name, info in frameworks.items():
        print(f"\nã€{name}ã€‘")
        print(f"ç‰¹ç‚¹: {', '.join(info['ç‰¹ç‚¹'])}")
        print(f"ä¼˜åŠ¿: {', '.join(info['ä¼˜åŠ¿'])}")
        print(f"é€‚ç”¨: {', '.join(info['é€‚ç”¨'])}")

def learning_roadmap():
    """æ·±åº¦å­¦ä¹ å­¦ä¹ è·¯çº¿å›¾"""
    print("\n=== æ·±åº¦å­¦ä¹ å­¦ä¹ è·¯çº¿å›¾ ===")
    
    roadmap = {
        "åŸºç¡€é˜¶æ®µ (1-2ä¸ªæœˆ)": [
            "ç†è§£ç¥ç»ç½‘ç»œåŸºæœ¬åŸç†",
            "æŒæ¡åå‘ä¼ æ’­ç®—æ³•",
            "ç†Ÿæ‚‰æ¿€æ´»å‡½æ•°å’ŒæŸå¤±å‡½æ•°",
            "å­¦ä¹ åŸºæœ¬çš„æ­£åˆ™åŒ–æŠ€æœ¯",
            "å®ç°ç®€å•çš„å¤šå±‚æ„ŸçŸ¥æœº"
        ],
        
        "è¿›é˜¶é˜¶æ®µ (2-3ä¸ªæœˆ)": [
            "å­¦ä¹ å·ç§¯ç¥ç»ç½‘ç»œ(CNN)",
            "ç†è§£å¾ªç¯ç¥ç»ç½‘ç»œ(RNN/LSTM)",
            "æŒæ¡ç°ä»£è®­ç»ƒæŠ€å·§",
            "å­¦ä¹ ä½¿ç”¨æ·±åº¦å­¦ä¹ æ¡†æ¶",
            "å®Œæˆå›¾åƒåˆ†ç±»å’Œæ–‡æœ¬åˆ†ç±»é¡¹ç›®"
        ],
        
        "é«˜çº§é˜¶æ®µ (3-4ä¸ªæœˆ)": [
            "å­¦ä¹ Transformeræ¶æ„",
            "ç†è§£æ³¨æ„åŠ›æœºåˆ¶",
            "æŒæ¡é¢„è®­ç»ƒå’Œå¾®è°ƒ",
            "å­¦ä¹ ç”Ÿæˆå¯¹æŠ—ç½‘ç»œ(GAN)",
            "å®Œæˆå¤æ‚çš„ç«¯åˆ°ç«¯é¡¹ç›®"
        ],
        
        "ä¸“å®¶é˜¶æ®µ (æŒç»­å­¦ä¹ )": [
            "è·Ÿè¸ªæœ€æ–°ç ”ç©¶è¿›å±•",
            "å­¦ä¹ ç‰¹å®šé¢†åŸŸçš„å…ˆè¿›æŠ€æœ¯",
            "å‚ä¸å¼€æºé¡¹ç›®è´¡çŒ®",
            "å‘è¡¨å­¦æœ¯è®ºæ–‡æˆ–æŠ€æœ¯åšå®¢",
            "è§£å†³å®é™…å·¥ä¸šé—®é¢˜"
        ]
    }
    
    for stage, tasks in roadmap.items():
        print(f"\nã€{stage}ã€‘")
        for i, task in enumerate(tasks, 1):
            print(f"{i}. {task}")

def practical_tips():
    """å®ç”¨æŠ€å·§å’Œå»ºè®®"""
    print("\n=== å®ç”¨æŠ€å·§å’Œå»ºè®® ===")
    
    print("ç¼–ç¨‹å®è·µæŠ€å·§ï¼š")
    print("â€¢ ä»ç®€å•æ¨¡å‹å¼€å§‹ï¼Œé€æ­¥å¢åŠ å¤æ‚åº¦")
    print("â€¢ ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹ä½œä¸ºèµ·ç‚¹")
    print("â€¢ é‡è§†æ•°æ®è´¨é‡å’Œé¢„å¤„ç†")
    print("â€¢ å»ºç«‹å®Œå–„çš„å®éªŒè®°å½•ç³»ç»Ÿ")
    print("â€¢ å¤šåšæ¶ˆèå®éªŒåˆ†ææ¨¡å‹æ€§èƒ½")
    print()
    
    print("è°ƒè¯•æŠ€å·§ï¼š")
    print("â€¢ æ£€æŸ¥æ•°æ®åŠ è½½å’Œé¢„å¤„ç†æµç¨‹")
    print("â€¢ éªŒè¯æ¨¡å‹è¾“å‡ºå½¢çŠ¶å’Œæ•°å€¼èŒƒå›´")
    print("â€¢ ç›‘æ§æ¢¯åº¦å¤§å°å’Œåˆ†å¸ƒ")
    print("â€¢ å¯è§†åŒ–ä¸­é—´å±‚ç‰¹å¾å›¾")
    print("â€¢ å¯¹æ¯”ç®€åŒ–ç‰ˆæœ¬çš„æ¨¡å‹æ€§èƒ½")
    print()
    
    print("æ€§èƒ½ä¼˜åŒ–ï¼š")
    print("â€¢ ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ")
    print("â€¢ ä¼˜åŒ–æ•°æ®åŠ è½½æµæ°´çº¿")
    print("â€¢ åˆç†è®¾ç½®æ‰¹å¤§å°")
    print("â€¢ ä½¿ç”¨åˆ†å¸ƒå¼è®­ç»ƒ")
    print("â€¢ è€ƒè™‘æ¨¡å‹å‹ç¼©å’Œé‡åŒ–")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ æ·±åº¦å­¦ä¹ åŸºç¡€æ•™ç¨‹")
    print("=" * 50)
    
    deep_learning_introduction()
    deep_learning_architectures_overview()
    deep_network_challenges()
    modern_training_techniques()
    transfer_learning_concept()
    deep_learning_frameworks()
    learning_roadmap()
    practical_tips()
    
    print("\n" + "=" * 50)
    print("ğŸ¯ æ€»ç»“")
    print("æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„é‡è¦åˆ†æ”¯ï¼š")
    print("â€¢ ä½¿ç”¨å¤šå±‚ç¥ç»ç½‘ç»œå­¦ä¹ å¤æ‚æ¨¡å¼")
    print("â€¢ åœ¨å¤§æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜å¼‚")
    print("â€¢ éœ€è¦æŒæ¡ç°ä»£è®­ç»ƒæŠ€å·§")
    print("â€¢ å¹¿æ³›åº”ç”¨äºå„ä¸ªé¢†åŸŸ")
    print()
    print("å­¦ä¹ å»ºè®®ï¼š")
    print("â€¢ ç†è®ºä¸å®è·µå¹¶é‡")
    print("â€¢ å¤šåšé¡¹ç›®å·©å›ºçŸ¥è¯†")
    print("â€¢ å…³æ³¨æœ€æ–°ç ”ç©¶è¿›å±•")
    print("â€¢ å‚ä¸å¼€æºç¤¾åŒº")
    print()
    print("ä¸‹ä¸€æ­¥ï¼šé€‰æ‹©å…·ä½“æ–¹å‘æ·±å…¥å­¦ä¹ ï¼")
    print("æ¨èï¼šå…ˆå­¦ä¹ CNNå¤„ç†å›¾åƒï¼Œæˆ–å­¦ä¹ Transformerå¤„ç†æ–‡æœ¬")

if __name__ == "__main__":
    main()
